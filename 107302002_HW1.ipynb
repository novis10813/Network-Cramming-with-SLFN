{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath) -> None:\n",
    "        \n",
    "        # load csv data\n",
    "        data = pd.read_csv(filepath, header=None)\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "        \n",
    "        # feature scaling\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "        # convert to tensors\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('train_all_0.csv')\n",
    "\n",
    "# create data indices for train val split\n",
    "data_size = len(dataset)\n",
    "indices = list(range(data_size))\n",
    "split = int(np.floor(0.2 * data_size))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# create data loader\n",
    "train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=16, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_true):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_result_sum = (y_pred_tag == y_true).sum().float()\n",
    "    acc = correct_result_sum / y_true.shape[0]\n",
    "    acc = torch.round(acc*100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline\n",
    "def train(train_loader=train_loader, val_loader=val_loader, model=None, epochs=None, criterion=None, optimizer=None):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            \n",
    "            x, y = batch\n",
    "            \n",
    "            y_pred = model(x.to(device))\n",
    "            loss = criterion(y_pred, y.to(device).unsqueeze(1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc = binary_acc(y_pred, y.unsqueeze(1))\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "        \n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        train_acc = sum(train_accs) / len(train_accs)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        valid_loss = []\n",
    "        valid_accs = []\n",
    "        \n",
    "        for batch in val_loader:\n",
    "            x, y = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x.to(device))\n",
    "                \n",
    "                acc = binary_acc(y_pred, y.unsqueeze(1))\n",
    "                \n",
    "                valid_loss.append(loss.item())\n",
    "                valid_accs.append(acc)\n",
    "        \n",
    "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "        \n",
    "        print(f'[ {epoch+1}/{epochs} ] | train_loss = {train_loss:.5f}, train_acc = {train_acc:.5f}, val_loss = {valid_loss:.5f}, val_acc = {valid_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessary to use pytorch\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, train_loader, val_loader, k, weight='uniform'):\n",
    "        \n",
    "        # initialize data\n",
    "        self.X_train = train_loader.dataset.X\n",
    "        self.y_train = train_loader.dataset.y\n",
    "        \n",
    "        self.X_val = val_loader.dataset.X\n",
    "        self.y_val = val_loader.dataset.y\n",
    "        \n",
    "        self.n_neighbors = k\n",
    "        self.weight = weight\n",
    "        \n",
    "        self.n_classes = len(set((train_loader.dataset.y).numpy()))\n",
    "    \n",
    "    def euclidean_dist(self, a, b):\n",
    "        return torch.sqrt(torch.sum((a - b)**2, dim=0))\n",
    "    \n",
    "    def k_neighbors(self, return_dist=False):\n",
    "        \n",
    "        dist = []\n",
    "        neigh_ind = []\n",
    "        \n",
    "        point_dist = [self.euclidean_dist(x_val, self.X_train) for x_val in self.X_val]\n",
    "        \n",
    "        for row in point_dist:\n",
    "            enum_neigh = enumerate(row)\n",
    "            sorted_neigh = sorted(enum_neigh,\n",
    "                                  key=lambda x: x[1])[:self.n_neighbors]\n",
    "            \n",
    "            ind_list = [tup[0] for tup in sorted_neigh]\n",
    "            dist_list = [tup[1] for tup in sorted_neigh]\n",
    "            \n",
    "            dist.append(dist_list)\n",
    "            neigh_ind.append(ind_list)\n",
    "            \n",
    "        if return_dist:\n",
    "            return np.array(dist), np.array(neigh_ind)\n",
    "        \n",
    "        return np.array(neigh_ind)\n",
    "    \n",
    "    def predict(self):\n",
    "        \n",
    "        if self.weight == 'uniform':\n",
    "            neighbors = self.k_neighbors(False)\n",
    "            y_pred = np.array([\n",
    "                np.argmax(np.bincount(self.y_train[neighbor]))\n",
    "                for neighbor in neighbors\n",
    "            ])\n",
    "            \n",
    "            return y_pred\n",
    "        \n",
    "        if self.weight == 'distance':\n",
    "            dist, neigh_ind = self.k_neighbors(True)\n",
    "            \n",
    "            inv_dist = 1 / dist\n",
    "            mean_inv_dist = inv_dist / np.sum(inv_dist, axis=1)[:, np.newaxis]\n",
    "            \n",
    "            prob = []\n",
    "            \n",
    "            for i, row in enumerate(mean_inv_dist):\n",
    "                row_pred = self.y_train[neigh_ind[i]]\n",
    "                \n",
    "                for k in range(self.n_classes):\n",
    "                    indices = np.where(row_pred == k)\n",
    "                    prob_ind = np.sum(row[indices])\n",
    "                    prob.append(np.array(prob_ind))\n",
    "            \n",
    "            predict_prob = np.array(prob).reshape(self.X_val.shape[0], self.n_classes)\n",
    "            y_pred = np.array([np.argmax(item) for item in predict_prob])\n",
    "            \n",
    "            return y_pred\n",
    "    \n",
    "    def score(self):\n",
    "        y_pred = self.predict()\n",
    "        return float(sum(y_pred == np.array(self.y_val))) / float(len(self.y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3958333333333333\n"
     ]
    }
   ],
   "source": [
    "knn_classifier = KNN(train_loader=train_loader, val_loader=val_loader, k=2, weight='distance')\n",
    "accuracy = knn_classifier.score()\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(output, label):\n",
    "    num_labels = len(label)\n",
    "    corrects = output[range(num_labels), label].unsqueeze(0).T\n",
    "    \n",
    "    margin = 1.0\n",
    "    margins = output - corrects + margin\n",
    "    loss = torch.sum(torch.max(margins, 1)[0]) / num_labels\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10 ] | train_loss = 1.34393, train_acc = 0.55208, val_loss = 1.27071, val_acc = 0.57292\n",
      "[ 2/10 ] | train_loss = 1.25896, train_acc = 0.57812, val_loss = 1.19379, val_acc = 0.60417\n",
      "[ 3/10 ] | train_loss = 1.19855, train_acc = 0.59635, val_loss = 1.26833, val_acc = 0.61458\n",
      "[ 4/10 ] | train_loss = 1.15551, train_acc = 0.59635, val_loss = 1.09382, val_acc = 0.61458\n",
      "[ 5/10 ] | train_loss = 1.12406, train_acc = 0.63021, val_loss = 1.08071, val_acc = 0.62500\n",
      "[ 6/10 ] | train_loss = 1.10359, train_acc = 0.65625, val_loss = 1.07573, val_acc = 0.65625\n",
      "[ 7/10 ] | train_loss = 1.08545, train_acc = 0.67448, val_loss = 1.02714, val_acc = 0.68750\n",
      "[ 8/10 ] | train_loss = 1.06998, train_acc = 0.69792, val_loss = 1.05933, val_acc = 0.70833\n",
      "[ 9/10 ] | train_loss = 1.05790, train_acc = 0.73698, val_loss = 1.06002, val_acc = 0.77083\n",
      "[ 10/10 ] | train_loss = 1.04782, train_acc = 0.77604, val_loss = 1.10647, val_acc = 0.77083\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "device = torch.device('cpu')\n",
    "model = nn.Linear(12, 2).to(device)\n",
    "criterion = hinge_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "train(train_loader, val_loader, model, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftmaxClassifier, self).__init__()\n",
    "        self.layer_1 = nn.Linear(12, 2) \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10 ] | train_loss = 0.55169, train_acc = 0.81771, val_loss = 0.48633, val_acc = 0.85417\n",
      "[ 2/10 ] | train_loss = 0.53065, train_acc = 0.83594, val_loss = 0.52339, val_acc = 0.86458\n",
      "[ 3/10 ] | train_loss = 0.51362, train_acc = 0.84115, val_loss = 0.57234, val_acc = 0.86458\n",
      "[ 4/10 ] | train_loss = 0.49812, train_acc = 0.84896, val_loss = 0.41153, val_acc = 0.86458\n",
      "[ 5/10 ] | train_loss = 0.48502, train_acc = 0.84115, val_loss = 0.42347, val_acc = 0.87500\n",
      "[ 6/10 ] | train_loss = 0.47260, train_acc = 0.84896, val_loss = 0.44053, val_acc = 0.87500\n",
      "[ 7/10 ] | train_loss = 0.46226, train_acc = 0.85156, val_loss = 0.47217, val_acc = 0.86458\n",
      "[ 8/10 ] | train_loss = 0.45225, train_acc = 0.85417, val_loss = 0.45541, val_acc = 0.86458\n",
      "[ 9/10 ] | train_loss = 0.44400, train_acc = 0.85677, val_loss = 0.46128, val_acc = 0.87500\n",
      "[ 10/10 ] | train_loss = 0.43462, train_acc = 0.86458, val_loss = 0.50207, val_acc = 0.87500\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "device = torch.device('cpu')\n",
    "model = SoftmaxClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "train(train_loader, val_loader, model, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(12, 128) \n",
    "        self.layer_out = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/50 ] | train_loss = 0.55802, train_acc = 80.12500, val_loss = 0.56368, val_acc = 80.33334\n",
      "[ 2/50 ] | train_loss = 0.47497, train_acc = 80.50000, val_loss = 0.59813, val_acc = 80.33334\n",
      "[ 3/50 ] | train_loss = 0.42741, train_acc = 82.12500, val_loss = 0.54487, val_acc = 84.50000\n",
      "[ 4/50 ] | train_loss = 0.39210, train_acc = 84.25000, val_loss = 0.16625, val_acc = 85.83334\n",
      "[ 5/50 ] | train_loss = 0.36651, train_acc = 85.66666, val_loss = 0.43186, val_acc = 85.50000\n",
      "[ 6/50 ] | train_loss = 0.34734, train_acc = 86.12500, val_loss = 0.43155, val_acc = 85.50000\n",
      "[ 7/50 ] | train_loss = 0.33393, train_acc = 86.37500, val_loss = 0.41545, val_acc = 84.66666\n",
      "[ 8/50 ] | train_loss = 0.32045, train_acc = 86.91666, val_loss = 0.42641, val_acc = 85.50000\n",
      "[ 9/50 ] | train_loss = 0.31034, train_acc = 87.37500, val_loss = 0.27395, val_acc = 85.50000\n",
      "[ 10/50 ] | train_loss = 0.30266, train_acc = 88.25000, val_loss = 0.41372, val_acc = 85.66666\n",
      "[ 11/50 ] | train_loss = 0.29385, train_acc = 89.25000, val_loss = 0.11944, val_acc = 85.66666\n",
      "[ 12/50 ] | train_loss = 0.28706, train_acc = 88.75000, val_loss = 0.25979, val_acc = 86.66666\n",
      "[ 13/50 ] | train_loss = 0.28058, train_acc = 90.00000, val_loss = 0.32214, val_acc = 86.50000\n",
      "[ 14/50 ] | train_loss = 0.27688, train_acc = 88.95834, val_loss = 0.20579, val_acc = 86.66666\n",
      "[ 15/50 ] | train_loss = 0.27019, train_acc = 89.95834, val_loss = 0.11362, val_acc = 87.83334\n",
      "[ 16/50 ] | train_loss = 0.26468, train_acc = 90.87500, val_loss = 0.19413, val_acc = 86.66666\n",
      "[ 17/50 ] | train_loss = 0.25795, train_acc = 90.83334, val_loss = 0.09516, val_acc = 86.66666\n",
      "[ 18/50 ] | train_loss = 0.25570, train_acc = 91.33334, val_loss = 0.15539, val_acc = 86.66666\n",
      "[ 19/50 ] | train_loss = 0.25439, train_acc = 90.75000, val_loss = 0.11599, val_acc = 86.66666\n",
      "[ 20/50 ] | train_loss = 0.24646, train_acc = 91.66666, val_loss = 0.23418, val_acc = 86.50000\n",
      "[ 21/50 ] | train_loss = 0.24087, train_acc = 91.87500, val_loss = 0.14569, val_acc = 86.50000\n",
      "[ 22/50 ] | train_loss = 0.23704, train_acc = 91.87500, val_loss = 0.21541, val_acc = 87.66666\n",
      "[ 23/50 ] | train_loss = 0.23378, train_acc = 91.83334, val_loss = 0.10064, val_acc = 85.66666\n",
      "[ 24/50 ] | train_loss = 0.23007, train_acc = 92.16666, val_loss = 0.09971, val_acc = 85.50000\n",
      "[ 25/50 ] | train_loss = 0.22622, train_acc = 91.87500, val_loss = 0.20846, val_acc = 86.66666\n",
      "[ 26/50 ] | train_loss = 0.22122, train_acc = 92.04166, val_loss = 0.11509, val_acc = 86.66666\n",
      "[ 27/50 ] | train_loss = 0.22142, train_acc = 91.91666, val_loss = 0.45291, val_acc = 87.83334\n",
      "[ 28/50 ] | train_loss = 0.21792, train_acc = 92.33334, val_loss = 0.10387, val_acc = 86.66666\n",
      "[ 29/50 ] | train_loss = 0.21332, train_acc = 92.66666, val_loss = 0.12246, val_acc = 87.66666\n",
      "[ 30/50 ] | train_loss = 0.21135, train_acc = 92.58334, val_loss = 0.10037, val_acc = 86.66666\n",
      "[ 31/50 ] | train_loss = 0.20572, train_acc = 93.12500, val_loss = 0.17015, val_acc = 87.66666\n",
      "[ 32/50 ] | train_loss = 0.20369, train_acc = 93.66666, val_loss = 0.15024, val_acc = 87.83334\n",
      "[ 33/50 ] | train_loss = 0.20365, train_acc = 92.83334, val_loss = 0.12561, val_acc = 87.50000\n",
      "[ 34/50 ] | train_loss = 0.19837, train_acc = 93.87500, val_loss = 0.44537, val_acc = 87.66666\n",
      "[ 35/50 ] | train_loss = 0.19583, train_acc = 93.70834, val_loss = 0.14091, val_acc = 86.50000\n",
      "[ 36/50 ] | train_loss = 0.19330, train_acc = 93.41666, val_loss = 0.10842, val_acc = 87.66666\n",
      "[ 37/50 ] | train_loss = 0.19174, train_acc = 93.70834, val_loss = 0.11144, val_acc = 88.66666\n",
      "[ 38/50 ] | train_loss = 0.18931, train_acc = 93.91666, val_loss = 0.12140, val_acc = 89.83334\n",
      "[ 39/50 ] | train_loss = 0.18499, train_acc = 93.58334, val_loss = 0.17608, val_acc = 90.83334\n",
      "[ 40/50 ] | train_loss = 0.18257, train_acc = 93.91666, val_loss = 0.13092, val_acc = 89.83334\n",
      "[ 41/50 ] | train_loss = 0.18071, train_acc = 94.12500, val_loss = 0.07714, val_acc = 89.66666\n",
      "[ 42/50 ] | train_loss = 0.17789, train_acc = 93.87500, val_loss = 0.14840, val_acc = 91.83334\n",
      "[ 43/50 ] | train_loss = 0.17488, train_acc = 94.16666, val_loss = 0.26873, val_acc = 90.83334\n",
      "[ 44/50 ] | train_loss = 0.17460, train_acc = 94.16666, val_loss = 0.10020, val_acc = 90.83334\n",
      "[ 45/50 ] | train_loss = 0.16997, train_acc = 94.41666, val_loss = 0.09381, val_acc = 90.83334\n",
      "[ 46/50 ] | train_loss = 0.16759, train_acc = 94.45834, val_loss = 0.05013, val_acc = 92.00000\n",
      "[ 47/50 ] | train_loss = 0.16520, train_acc = 94.45834, val_loss = 0.10991, val_acc = 90.83334\n",
      "[ 48/50 ] | train_loss = 0.16461, train_acc = 94.75000, val_loss = 0.25087, val_acc = 91.83334\n",
      "[ 49/50 ] | train_loss = 0.16268, train_acc = 94.95834, val_loss = 0.17764, val_acc = 90.83334\n",
      "[ 50/50 ] | train_loss = 0.16031, train_acc = 94.70834, val_loss = 0.09009, val_acc = 91.00000\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "device = torch.device('cpu')\n",
    "model = BinaryClassification().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "train(train_loader, val_loader, model, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "219da6a50c866249bdfc07e8ee29701a3e2568a26ff21cc98a0eb284d1611ca6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
