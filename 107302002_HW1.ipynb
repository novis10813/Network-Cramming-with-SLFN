{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath) -> None:\n",
    "        \n",
    "        # load csv data\n",
    "        data = pd.read_csv(filepath, header=None)\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "        \n",
    "        # feature scaling\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "        # convert to tensors\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('train_all_0.csv')\n",
    "\n",
    "# create data indices for train val split\n",
    "data_size = len(dataset)\n",
    "indices = list(range(data_size))\n",
    "split = int(np.floor(0.2 * data_size))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# create data loader\n",
    "train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=16, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline\n",
    "def train(train_loader=train_loader, val_loader=val_loader, model=None, epochs=None, criterion=None, optimizer=None):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            \n",
    "            x, y = batch\n",
    "            \n",
    "            logits = model(x.to(device))\n",
    "            loss = criterion(logits, y.to(device))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()\n",
    "            train_loss.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "        \n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        train_acc = sum(train_accs) / len(train_accs)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        valid_loss = []\n",
    "        valid_accs = []\n",
    "        \n",
    "        for batch in val_loader:\n",
    "            imgs, labels = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model(imgs.to(device))\n",
    "                \n",
    "                acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "                \n",
    "                valid_loss.append(loss.item())\n",
    "                valid_accs.append(acc)\n",
    "        \n",
    "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "        \n",
    "        print(f'[ {epoch+1}/{epochs} ] | train_loss = {train_loss:.5f}, train_acc = {train_acc:.5f}, val_loss = {valid_loss:.5f}, val_acc = {valid_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessary to use pytorch\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, train_loader, val_loader, k, weight='uniform'):\n",
    "        \n",
    "        # initialize data\n",
    "        self.X_train = train_loader.dataset.X\n",
    "        self.y_train = train_loader.dataset.y\n",
    "        \n",
    "        self.X_val = val_loader.dataset.X\n",
    "        self.y_val = val_loader.dataset.y\n",
    "        \n",
    "        self.n_neighbors = k\n",
    "        self.weight = weight\n",
    "        \n",
    "        self.n_classes = len(set((train_loader.dataset.y).numpy()))\n",
    "    \n",
    "    def euclidean_dist(self, a, b):\n",
    "        return torch.sqrt(torch.sum((a - b)**2, dim=0))\n",
    "    \n",
    "    def k_neighbors(self, return_dist=False):\n",
    "        \n",
    "        dist = []\n",
    "        neigh_ind = []\n",
    "        \n",
    "        point_dist = [self.euclidean_dist(x_val, self.X_train) for x_val in self.X_val]\n",
    "        \n",
    "        for row in point_dist:\n",
    "            enum_neigh = enumerate(row)\n",
    "            sorted_neigh = sorted(enum_neigh,\n",
    "                                  key=lambda x: x[1])[:self.n_neighbors]\n",
    "            \n",
    "            ind_list = [tup[0] for tup in sorted_neigh]\n",
    "            dist_list = [tup[1] for tup in sorted_neigh]\n",
    "            \n",
    "            dist.append(dist_list)\n",
    "            neigh_ind.append(ind_list)\n",
    "            \n",
    "        if return_dist:\n",
    "            return np.array(dist), np.array(neigh_ind)\n",
    "        \n",
    "        return np.array(neigh_ind)\n",
    "    \n",
    "    def predict(self):\n",
    "        \n",
    "        if self.weight == 'uniform':\n",
    "            neighbors = self.k_neighbors(False)\n",
    "            y_pred = np.array([\n",
    "                np.argmax(np.bincount(self.y_train[neighbor]))\n",
    "                for neighbor in neighbors\n",
    "            ])\n",
    "            \n",
    "            return y_pred\n",
    "        \n",
    "        if self.weight == 'distance':\n",
    "            dist, neigh_ind = self.k_neighbors(True)\n",
    "            \n",
    "            inv_dist = 1 / dist\n",
    "            mean_inv_dist = inv_dist / np.sum(inv_dist, axis=1)[:, np.newaxis]\n",
    "            \n",
    "            prob = []\n",
    "            \n",
    "            for i, row in enumerate(mean_inv_dist):\n",
    "                row_pred = self.y_train[neigh_ind[i]]\n",
    "                \n",
    "                for k in range(self.n_classes):\n",
    "                    indices = np.where(row_pred == k)\n",
    "                    prob_ind = np.sum(row[indices])\n",
    "                    prob.append(np.array(prob_ind))\n",
    "            \n",
    "            predict_prob = np.array(prob).reshape(self.X_val.shape[0], self.n_classes)\n",
    "            y_pred = np.array([np.argmax(item) for item in predict_prob])\n",
    "            \n",
    "            return y_pred\n",
    "    \n",
    "    def score(self):\n",
    "        y_pred = self.predict()\n",
    "        return float(sum(y_pred == np.array(self.y_val))) / float(len(self.y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3958333333333333\n"
     ]
    }
   ],
   "source": [
    "knn_classifier = KNN(train_loader=train_loader, val_loader=val_loader, k=2, weight='distance')\n",
    "accuracy = knn_classifier.score()\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(output, label):\n",
    "    num_labels = len(label)\n",
    "    corrects = output[range(num_labels), label].unsqueeze(0).T\n",
    "    \n",
    "    margin = 1.0\n",
    "    margins = output - corrects + margin\n",
    "    loss = torch.sum(torch.max(margins, 1)[0]) / num_labels\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10 ] | train_loss = 1.34393, train_acc = 0.55208, val_loss = 1.27071, val_acc = 0.57292\n",
      "[ 2/10 ] | train_loss = 1.25896, train_acc = 0.57812, val_loss = 1.19379, val_acc = 0.60417\n",
      "[ 3/10 ] | train_loss = 1.19855, train_acc = 0.59635, val_loss = 1.26833, val_acc = 0.61458\n",
      "[ 4/10 ] | train_loss = 1.15551, train_acc = 0.59635, val_loss = 1.09382, val_acc = 0.61458\n",
      "[ 5/10 ] | train_loss = 1.12406, train_acc = 0.63021, val_loss = 1.08071, val_acc = 0.62500\n",
      "[ 6/10 ] | train_loss = 1.10359, train_acc = 0.65625, val_loss = 1.07573, val_acc = 0.65625\n",
      "[ 7/10 ] | train_loss = 1.08545, train_acc = 0.67448, val_loss = 1.02714, val_acc = 0.68750\n",
      "[ 8/10 ] | train_loss = 1.06998, train_acc = 0.69792, val_loss = 1.05933, val_acc = 0.70833\n",
      "[ 9/10 ] | train_loss = 1.05790, train_acc = 0.73698, val_loss = 1.06002, val_acc = 0.77083\n",
      "[ 10/10 ] | train_loss = 1.04782, train_acc = 0.77604, val_loss = 1.10647, val_acc = 0.77083\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "device = torch.device('cpu')\n",
    "model = nn.Linear(12, 2).to(device)\n",
    "criterion = hinge_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "train(train_loader, val_loader, model, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftmaxClassifier, self).__init__()\n",
    "        self.layer_1 = nn.Linear(12, 2) \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10 ] | train_loss = 0.55169, train_acc = 0.81771, val_loss = 0.48633, val_acc = 0.85417\n",
      "[ 2/10 ] | train_loss = 0.53065, train_acc = 0.83594, val_loss = 0.52339, val_acc = 0.86458\n",
      "[ 3/10 ] | train_loss = 0.51362, train_acc = 0.84115, val_loss = 0.57234, val_acc = 0.86458\n",
      "[ 4/10 ] | train_loss = 0.49812, train_acc = 0.84896, val_loss = 0.41153, val_acc = 0.86458\n",
      "[ 5/10 ] | train_loss = 0.48502, train_acc = 0.84115, val_loss = 0.42347, val_acc = 0.87500\n",
      "[ 6/10 ] | train_loss = 0.47260, train_acc = 0.84896, val_loss = 0.44053, val_acc = 0.87500\n",
      "[ 7/10 ] | train_loss = 0.46226, train_acc = 0.85156, val_loss = 0.47217, val_acc = 0.86458\n",
      "[ 8/10 ] | train_loss = 0.45225, train_acc = 0.85417, val_loss = 0.45541, val_acc = 0.86458\n",
      "[ 9/10 ] | train_loss = 0.44400, train_acc = 0.85677, val_loss = 0.46128, val_acc = 0.87500\n",
      "[ 10/10 ] | train_loss = 0.43462, train_acc = 0.86458, val_loss = 0.50207, val_acc = 0.87500\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "device = torch.device('cpu')\n",
    "model = SoftmaxClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "train(train_loader, val_loader, model, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(12, 128) \n",
    "        self.layer_out = nn.Linear(128, 2) \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10 ] | train_loss = 0.58412, train_acc = 0.77083, val_loss = 0.50714, val_acc = 0.79167\n",
      "[ 2/10 ] | train_loss = 0.44715, train_acc = 0.81250, val_loss = 0.40733, val_acc = 0.81250\n",
      "[ 3/10 ] | train_loss = 0.39121, train_acc = 0.84635, val_loss = 0.19099, val_acc = 0.84375\n",
      "[ 4/10 ] | train_loss = 0.35685, train_acc = 0.86458, val_loss = 0.48609, val_acc = 0.84375\n",
      "[ 5/10 ] | train_loss = 0.33304, train_acc = 0.86458, val_loss = 0.44749, val_acc = 0.84375\n",
      "[ 6/10 ] | train_loss = 0.31860, train_acc = 0.87500, val_loss = 0.42144, val_acc = 0.86458\n",
      "[ 7/10 ] | train_loss = 0.30729, train_acc = 0.88802, val_loss = 0.18491, val_acc = 0.86458\n",
      "[ 8/10 ] | train_loss = 0.29652, train_acc = 0.88802, val_loss = 0.21103, val_acc = 0.87500\n",
      "[ 9/10 ] | train_loss = 0.28481, train_acc = 0.89844, val_loss = 0.54152, val_acc = 0.86458\n",
      "[ 10/10 ] | train_loss = 0.27667, train_acc = 0.90104, val_loss = 0.51886, val_acc = 0.87500\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "device = torch.device('cpu')\n",
    "model = BinaryClassification().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "train(train_loader, val_loader, model, epochs, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac3da779536756720bc930bbdcbe3b303a716c4190960bb8b007750e7b6b7c5d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
