{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath) -> None:\n",
    "        \n",
    "        # load csv data\n",
    "        data = pd.read_csv(filepath, header=None)\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "        \n",
    "        # feature scaling\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "        # convert to tensors\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('data/train_all_0.csv')\n",
    "\n",
    "# create data indices for train val split\n",
    "data_size = len(dataset)\n",
    "indices = list(range(data_size))\n",
    "split = int(np.floor(0.2 * data_size))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# create data loader\n",
    "train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=16, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_true):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_result_sum = (y_pred_tag == y_true).sum().float()\n",
    "    acc = correct_result_sum / y_true.shape[0]\n",
    "    acc = torch.round(acc*100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline\n",
    "def train(train_loader=train_loader, val_loader=val_loader, model=None, epochs=None, criterion=None, optimizer=None):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            \n",
    "            x, y = batch\n",
    "            \n",
    "            y_pred = model(x.to(device))\n",
    "            loss = criterion(y_pred, y.to(device).view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc = binary_acc(y_pred, y.view(-1))\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "        \n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        train_acc = sum(train_accs) / len(train_accs)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        valid_loss = []\n",
    "        valid_accs = []\n",
    "        \n",
    "        for batch in val_loader:\n",
    "            x, y = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x.to(device))\n",
    "                \n",
    "                acc = binary_acc(y_pred, y.view(-1))\n",
    "                \n",
    "                valid_loss.append(loss.item())\n",
    "                valid_accs.append(acc)\n",
    "        \n",
    "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "        \n",
    "        print(f'[ {epoch+1}/{epochs} ] | train_loss = {train_loss:.5f}, train_acc = {train_acc:.5f}, val_loss = {valid_loss:.5f}, val_acc = {valid_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessary to use pytorch\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, train_loader, val_loader, k, weight='uniform'):\n",
    "        \n",
    "        # initialize data\n",
    "        self.X_train = train_loader.dataset.X\n",
    "        self.y_train = train_loader.dataset.y\n",
    "        \n",
    "        self.X_val = val_loader.dataset.X\n",
    "        self.y_val = val_loader.dataset.y\n",
    "        \n",
    "        self.n_neighbors = k\n",
    "        self.weight = weight\n",
    "        \n",
    "        self.n_classes = len(set((train_loader.dataset.y).numpy()))\n",
    "    \n",
    "    def euclidean_dist(self, a, b):\n",
    "        return torch.sqrt(torch.sum((a - b)**2, dim=0))\n",
    "    \n",
    "    def k_neighbors(self, return_dist=False):\n",
    "        \n",
    "        dist = []\n",
    "        neigh_ind = []\n",
    "        \n",
    "        point_dist = [self.euclidean_dist(x_val, self.X_train) for x_val in self.X_val]\n",
    "        \n",
    "        for row in point_dist:\n",
    "            enum_neigh = enumerate(row)\n",
    "            sorted_neigh = sorted(enum_neigh,\n",
    "                                  key=lambda x: x[1])[:self.n_neighbors]\n",
    "            \n",
    "            ind_list = [tup[0] for tup in sorted_neigh]\n",
    "            dist_list = [tup[1] for tup in sorted_neigh]\n",
    "            \n",
    "            dist.append(dist_list)\n",
    "            neigh_ind.append(ind_list)\n",
    "            \n",
    "        if return_dist:\n",
    "            return np.array(dist), np.array(neigh_ind)\n",
    "        \n",
    "        return np.array(neigh_ind)\n",
    "    \n",
    "    def predict(self):\n",
    "        \n",
    "        if self.weight == 'uniform':\n",
    "            neighbors = self.k_neighbors(False)\n",
    "            y_pred = np.array([\n",
    "                np.argmax(np.bincount(self.y_train[neighbor]))\n",
    "                for neighbor in neighbors\n",
    "            ])\n",
    "            \n",
    "            return y_pred\n",
    "        \n",
    "        if self.weight == 'distance':\n",
    "            dist, neigh_ind = self.k_neighbors(True)\n",
    "            \n",
    "            inv_dist = 1 / dist\n",
    "            mean_inv_dist = inv_dist / np.sum(inv_dist, axis=1)[:, np.newaxis]\n",
    "            \n",
    "            prob = []\n",
    "            \n",
    "            for i, row in enumerate(mean_inv_dist):\n",
    "                row_pred = self.y_train[neigh_ind[i]]\n",
    "                \n",
    "                for k in range(self.n_classes):\n",
    "                    indices = np.where(row_pred == k)\n",
    "                    prob_ind = np.sum(row[indices])\n",
    "                    prob.append(np.array(prob_ind))\n",
    "            \n",
    "            predict_prob = np.array(prob).reshape(self.X_val.shape[0], self.n_classes)\n",
    "            y_pred = np.array([np.argmax(item) for item in predict_prob])\n",
    "            \n",
    "            return y_pred\n",
    "    \n",
    "    def score(self):\n",
    "        y_pred = self.predict()\n",
    "        return float(sum(y_pred == np.array(self.y_val))) / float(len(self.y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3958333333333333\n"
     ]
    }
   ],
   "source": [
    "knn_classifier = KNN(train_loader=train_loader, val_loader=val_loader, k=2, weight='distance')\n",
    "accuracy = knn_classifier.score()\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(output, label):\n",
    "    num_labels = len(label)\n",
    "    corrects = output[range(num_labels), label].unsqueeze(0).T\n",
    "    \n",
    "    margin = 1.0\n",
    "    margins = output - corrects + margin\n",
    "    loss = torch.sum(torch.max(margins, 1)[0]) / num_labels\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1848/2065353156.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhinge_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1848/3173386938.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, val_loader, model, epochs, criterion, optimizer)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1848/2137315492.py\u001b[0m in \u001b[0;36mhinge_loss\u001b[1;34m(output, label)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhinge_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcorrects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmargin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "device = torch.device('cpu')\n",
    "model = nn.Linear(12, 2).to(device)\n",
    "criterion = hinge_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "train(train_loader, val_loader, model, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftmaxClassifier, self).__init__()\n",
    "        self.layer_1 = nn.Linear(12, 2) \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1848/3578814352.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1848/1115518745.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, val_loader, model, epochs, criterion, optimizer)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dddru\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dddru\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dddru\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2844\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2846\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "device = torch.device('cpu')\n",
    "model = SoftmaxClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "train(train_loader, val_loader, model, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(12, 128) \n",
    "        self.layer_out = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/50 ] | train_loss = 0.55802, train_acc = 80.12500, val_loss = 0.56368, val_acc = 80.33334\n",
      "[ 2/50 ] | train_loss = 0.47497, train_acc = 80.50000, val_loss = 0.59813, val_acc = 80.33334\n",
      "[ 3/50 ] | train_loss = 0.42741, train_acc = 82.12500, val_loss = 0.54487, val_acc = 84.50000\n",
      "[ 4/50 ] | train_loss = 0.39210, train_acc = 84.25000, val_loss = 0.16625, val_acc = 85.83334\n",
      "[ 5/50 ] | train_loss = 0.36651, train_acc = 85.66666, val_loss = 0.43186, val_acc = 85.50000\n",
      "[ 6/50 ] | train_loss = 0.34734, train_acc = 86.12500, val_loss = 0.43155, val_acc = 85.50000\n",
      "[ 7/50 ] | train_loss = 0.33393, train_acc = 86.37500, val_loss = 0.41545, val_acc = 84.66666\n",
      "[ 8/50 ] | train_loss = 0.32045, train_acc = 86.91666, val_loss = 0.42641, val_acc = 85.50000\n",
      "[ 9/50 ] | train_loss = 0.31034, train_acc = 87.37500, val_loss = 0.27395, val_acc = 85.50000\n",
      "[ 10/50 ] | train_loss = 0.30266, train_acc = 88.25000, val_loss = 0.41372, val_acc = 85.66666\n",
      "[ 11/50 ] | train_loss = 0.29385, train_acc = 89.25000, val_loss = 0.11944, val_acc = 85.66666\n",
      "[ 12/50 ] | train_loss = 0.28706, train_acc = 88.75000, val_loss = 0.25979, val_acc = 86.66666\n",
      "[ 13/50 ] | train_loss = 0.28058, train_acc = 90.00000, val_loss = 0.32214, val_acc = 86.50000\n",
      "[ 14/50 ] | train_loss = 0.27688, train_acc = 88.95834, val_loss = 0.20579, val_acc = 86.66666\n",
      "[ 15/50 ] | train_loss = 0.27019, train_acc = 89.95834, val_loss = 0.11362, val_acc = 87.83334\n",
      "[ 16/50 ] | train_loss = 0.26468, train_acc = 90.87500, val_loss = 0.19413, val_acc = 86.66666\n",
      "[ 17/50 ] | train_loss = 0.25795, train_acc = 90.83334, val_loss = 0.09516, val_acc = 86.66666\n",
      "[ 18/50 ] | train_loss = 0.25570, train_acc = 91.33334, val_loss = 0.15539, val_acc = 86.66666\n",
      "[ 19/50 ] | train_loss = 0.25439, train_acc = 90.75000, val_loss = 0.11599, val_acc = 86.66666\n",
      "[ 20/50 ] | train_loss = 0.24646, train_acc = 91.66666, val_loss = 0.23418, val_acc = 86.50000\n",
      "[ 21/50 ] | train_loss = 0.24087, train_acc = 91.87500, val_loss = 0.14569, val_acc = 86.50000\n",
      "[ 22/50 ] | train_loss = 0.23704, train_acc = 91.87500, val_loss = 0.21541, val_acc = 87.66666\n",
      "[ 23/50 ] | train_loss = 0.23378, train_acc = 91.83334, val_loss = 0.10064, val_acc = 85.66666\n",
      "[ 24/50 ] | train_loss = 0.23007, train_acc = 92.16666, val_loss = 0.09971, val_acc = 85.50000\n",
      "[ 25/50 ] | train_loss = 0.22622, train_acc = 91.87500, val_loss = 0.20846, val_acc = 86.66666\n",
      "[ 26/50 ] | train_loss = 0.22122, train_acc = 92.04166, val_loss = 0.11509, val_acc = 86.66666\n",
      "[ 27/50 ] | train_loss = 0.22142, train_acc = 91.91666, val_loss = 0.45291, val_acc = 87.83334\n",
      "[ 28/50 ] | train_loss = 0.21792, train_acc = 92.33334, val_loss = 0.10387, val_acc = 86.66666\n",
      "[ 29/50 ] | train_loss = 0.21332, train_acc = 92.66666, val_loss = 0.12246, val_acc = 87.66666\n",
      "[ 30/50 ] | train_loss = 0.21135, train_acc = 92.58334, val_loss = 0.10037, val_acc = 86.66666\n",
      "[ 31/50 ] | train_loss = 0.20572, train_acc = 93.12500, val_loss = 0.17015, val_acc = 87.66666\n",
      "[ 32/50 ] | train_loss = 0.20369, train_acc = 93.66666, val_loss = 0.15024, val_acc = 87.83334\n",
      "[ 33/50 ] | train_loss = 0.20365, train_acc = 92.83334, val_loss = 0.12561, val_acc = 87.50000\n",
      "[ 34/50 ] | train_loss = 0.19837, train_acc = 93.87500, val_loss = 0.44537, val_acc = 87.66666\n",
      "[ 35/50 ] | train_loss = 0.19583, train_acc = 93.70834, val_loss = 0.14091, val_acc = 86.50000\n",
      "[ 36/50 ] | train_loss = 0.19330, train_acc = 93.41666, val_loss = 0.10842, val_acc = 87.66666\n",
      "[ 37/50 ] | train_loss = 0.19174, train_acc = 93.70834, val_loss = 0.11144, val_acc = 88.66666\n",
      "[ 38/50 ] | train_loss = 0.18931, train_acc = 93.91666, val_loss = 0.12140, val_acc = 89.83334\n",
      "[ 39/50 ] | train_loss = 0.18499, train_acc = 93.58334, val_loss = 0.17608, val_acc = 90.83334\n",
      "[ 40/50 ] | train_loss = 0.18257, train_acc = 93.91666, val_loss = 0.13092, val_acc = 89.83334\n",
      "[ 41/50 ] | train_loss = 0.18071, train_acc = 94.12500, val_loss = 0.07714, val_acc = 89.66666\n",
      "[ 42/50 ] | train_loss = 0.17789, train_acc = 93.87500, val_loss = 0.14840, val_acc = 91.83334\n",
      "[ 43/50 ] | train_loss = 0.17488, train_acc = 94.16666, val_loss = 0.26873, val_acc = 90.83334\n",
      "[ 44/50 ] | train_loss = 0.17460, train_acc = 94.16666, val_loss = 0.10020, val_acc = 90.83334\n",
      "[ 45/50 ] | train_loss = 0.16997, train_acc = 94.41666, val_loss = 0.09381, val_acc = 90.83334\n",
      "[ 46/50 ] | train_loss = 0.16759, train_acc = 94.45834, val_loss = 0.05013, val_acc = 92.00000\n",
      "[ 47/50 ] | train_loss = 0.16520, train_acc = 94.45834, val_loss = 0.10991, val_acc = 90.83334\n",
      "[ 48/50 ] | train_loss = 0.16461, train_acc = 94.75000, val_loss = 0.25087, val_acc = 91.83334\n",
      "[ 49/50 ] | train_loss = 0.16268, train_acc = 94.95834, val_loss = 0.17764, val_acc = 90.83334\n",
      "[ 50/50 ] | train_loss = 0.16031, train_acc = 94.70834, val_loss = 0.09009, val_acc = 91.00000\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "device = torch.device('cpu')\n",
    "model = BinaryClassification().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "train(train_loader, val_loader, model, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "219da6a50c866249bdfc07e8ee29701a3e2568a26ff21cc98a0eb284d1611ca6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
