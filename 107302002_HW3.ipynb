{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath) -> None:\n",
    "        \n",
    "        # load csv data\n",
    "        data = pd.read_csv(filepath, header=None)\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "        \n",
    "        # feature scaling\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "        # convert to tensors\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('train_all_0.csv')\n",
    "\n",
    "# create data indices for train val split\n",
    "data_size = len(dataset)\n",
    "indices = list(range(data_size))\n",
    "split = int(np.floor(0.2 * data_size))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# create data loader\n",
    "train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=16, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self, dropout=False, batch_norm=False):\n",
    "        '''Once at a time'''\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(12, 1024) \n",
    "        self.layer_out = nn.Linear(1024, 2) \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if dropout:\n",
    "            self.dropout_1 = nn.Dropout(0.2)\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.batch_norm_1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if self.dropout:\n",
    "            x = self.relu(self.layer_1(inputs))\n",
    "            x = self.dropout_1(x)\n",
    "            x = self.layer_out(x)\n",
    "        \n",
    "        elif self.dropout:\n",
    "            x = self.relu(self.layer_1(inputs))\n",
    "            x = self.batch_norm_1(x)\n",
    "            x = self.layer_out(x)\n",
    "            \n",
    "        else:\n",
    "            x = self.relu(self.layer_1(inputs))\n",
    "            x = self.layer_out(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline based on page 58\n",
    "def LG_UA_weight_tune(train_loader=train_loader, val_loader=val_loader, model=None, criterion=None, optimizer=None, device=None, loss_threshold=0.5, eta_threshold=0.0008):\n",
    "    print('Initialize weight-tuning LG UA model')\n",
    "    \n",
    "    previous_train_loss = 10000    \n",
    "\n",
    "    for epoch in itertools.count():\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        previous_model_params = model.state_dict()\n",
    "        stop_training = False\n",
    "        \n",
    "        while optimizer.param_groups[0]['lr'] > eta_threshold:\n",
    "            \n",
    "            train_loss = []\n",
    "            train_accs = []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                \n",
    "                x, y = batch\n",
    "                \n",
    "                logits = model(x.to(device))\n",
    "                loss = criterion(logits, y.to(device))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()\n",
    "                train_loss.append(loss.item())\n",
    "                train_accs.append(acc)\n",
    "            \n",
    "            train_loss = sum(train_loss) / len(train_loss)\n",
    "            train_acc = sum(train_accs) / len(train_accs)\n",
    "            \n",
    "            if train_loss < previous_train_loss:\n",
    "                optimizer.param_groups[0]['lr'] *= 1.2\n",
    "                previous_train_loss = train_loss\n",
    "                print(f'The previous training loss is: {previous_train_loss}')\n",
    "                break\n",
    "            \n",
    "            optimizer.param_groups[0]['lr'] *= 0.7\n",
    "            model.load_state_dict(previous_model_params)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'lr shrinking!, now the lr is: {current_lr}')\n",
    "            \n",
    "        else:\n",
    "            stop_training = True\n",
    "        \n",
    "        # Use try and except to detect whether the eta_threshold is set too high initially\n",
    "        try:        \n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_accs = []\n",
    "            \n",
    "            for batch in val_loader:\n",
    "                imgs, labels = batch\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = model(imgs.to(device))\n",
    "                    \n",
    "                    acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "                    valid_loss.append(loss.item())\n",
    "                    valid_accs.append(acc)\n",
    "            \n",
    "            valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "            valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "        \n",
    "            print(f'[ {epoch+1} ] | train_loss = {train_loss:.5f}, train_acc = {train_acc:.5f}, val_loss = {valid_loss:.5f}, val_acc = {valid_acc:.5f}')\n",
    "            \n",
    "        except UnboundLocalError:\n",
    "            print('Your eta_threshold is setting higher than your learning rate. Reset it with lower one!')\n",
    "        \n",
    "        # stopping criterion\n",
    "        if stop_training:\n",
    "            print('LG_UA_weight: Learning rate is smaller than the threshold, stop training. Unacceptable')\n",
    "            return True\n",
    "        \n",
    "        if train_loss < loss_threshold:\n",
    "            print('LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable')\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LG_UA_regularization(train_loader=train_loader, val_loader=val_loader, model=None, criterion=None, optimizer=None, device=None, loss_threshold=0.5, eta_threshold=0.0008, l2_lambda=0.001):\n",
    "    '''\n",
    "    Based on page 47, it should be L2 regularization and I can actually use \"weight_decay\" in pytorch optimizer.\n",
    "    But it's not fun so I will still implement L2 regularization by myself.\n",
    "    '''\n",
    "    previous_train_loss = 10000    \n",
    "\n",
    "    for epoch in itertools.count():\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        previous_model_params = model.state_dict()\n",
    "        stop_training = False\n",
    "        \n",
    "        while optimizer.param_groups[0]['lr'] > eta_threshold:\n",
    "            \n",
    "            train_loss = []\n",
    "            train_accs = []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                \n",
    "                x, y = batch\n",
    "                \n",
    "                logits = model(x.to(device))\n",
    "                loss = criterion(logits, y.to(device))\n",
    "                \n",
    "                # L2 regularization with normalized l2\n",
    "                L2_regularization = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                param_num = sum(p.numel() for p in model.parameters())\n",
    "                loss += (l2_lambda / param_num) * L2_regularization\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()\n",
    "                train_loss.append(loss.item())\n",
    "                train_accs.append(acc)\n",
    "            \n",
    "            max_train_loss = max(train_loss)\n",
    "            train_loss = sum(train_loss) / len(train_loss)\n",
    "            train_acc = sum(train_accs) / len(train_accs)\n",
    "            \n",
    "            \n",
    "            if train_loss <= previous_train_loss:\n",
    "                if max_train_loss < loss_threshold:\n",
    "                    optimizer.param_groups[0]['lr'] *= 1.2\n",
    "                    previous_train_loss = train_loss\n",
    "                    # print(f'The previous training loss is: {previous_train_loss}')\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    model.load_state_dict(previous_model_params)\n",
    "                    stop_training = True\n",
    "                    # print(f'max loss: {max_train_loss} | loss_threshold: {loss_threshold}')\n",
    "                    break\n",
    "            \n",
    "            optimizer.param_groups[0]['lr'] *= 0.7\n",
    "            model.load_state_dict(previous_model_params)\n",
    "            # current_lr = optimizer.param_groups[0]['lr']\n",
    "            # print(f'lr shrinking!, now the lr is: {current_lr}')\n",
    "            \n",
    "        else:\n",
    "            stop_training = True\n",
    "            model.load_state_dict(previous_model_params)\n",
    "            # print('learning rate < eta_threshold')\n",
    "        \n",
    "        # Use try and except to detect whether the eta_threshold is set too high initially\n",
    "        try:        \n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_accs = []\n",
    "            \n",
    "            for batch in val_loader:\n",
    "                imgs, labels = batch\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = model(imgs.to(device))\n",
    "                    \n",
    "                    acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "                    valid_loss.append(loss.item())\n",
    "                    valid_accs.append(acc)\n",
    "            \n",
    "            valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "            valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "        \n",
    "            print(f'[ {epoch+1} ] | train_loss = {train_loss:.5f}, train_acc = {train_acc:.5f}, val_loss = {valid_loss:.5f}, val_acc = {valid_acc:.5f}')\n",
    "            \n",
    "        except UnboundLocalError:\n",
    "            print('LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!')\n",
    "        \n",
    "        # stopping criterion\n",
    "        if stop_training:\n",
    "            print('LG_UA_reg: Restore previous model weights, stop training.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderUnstructured(prune.RandomUnstructured):\n",
    "    def __init__(self, amount=1, index=0):\n",
    "        super(OrderUnstructured, self).__init__(amount)\n",
    "        self.index = index\n",
    "        \n",
    "    def compute_mask(self, t, default_mask):\n",
    "        nparams_toprune = self.amount\n",
    "        mask = default_mask.clone(memory_format=torch.contiguous_format)\n",
    "        \n",
    "        if nparams_toprune != 0:  # k=0 not supported by torch.kthvalue\n",
    "            mask.view(-1)[self.index:self.amount+self.index] = 0\n",
    "        \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, index=0, amount=1):\n",
    "    new_model = copy.copy(model)\n",
    "    module_list = []\n",
    "    for name, module in new_model.layer_1.named_modules():\n",
    "        module_list += [(module, 'weight'), (module, 'bias')]\n",
    "    \n",
    "    prune.global_unstructured(\n",
    "    module_list,\n",
    "    pruning_method=OrderUnstructured,\n",
    "    amount=amount,\n",
    "    index=index)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def All_r_LG_UA_w_LG_UA(train_loader=train_loader,\n",
    "                        val_loader=val_loader,\n",
    "                        model=None,\n",
    "                        criterion=None,\n",
    "                        optimizer=None,\n",
    "                        device=None,\n",
    "                        loss_threshold=0.5,\n",
    "                        eta_threshold=0.2,\n",
    "                        l2_lambda=0.001,\n",
    "                        k=1,\n",
    "                        p=50):\n",
    "    \n",
    "    prune_index = 0\n",
    "    while k<p:\n",
    "        LG_UA_regularization(train_loader, val_loader, model, criterion, optimizer, device, loss_threshold, eta_threshold, l2_lambda)\n",
    "        saved_model = copy.deepcopy(model)\n",
    "        \n",
    "        new_model = prune_model(model, index=prune_index)\n",
    "        \n",
    "        unacceptable = LG_UA_weight_tune(train_loader, val_loader, new_model, criterion, optimizer, device, loss_threshold, eta_threshold*0.1)\n",
    "        \n",
    "        if unacceptable:\n",
    "            print('All : Restore Network')\n",
    "            model = saved_model\n",
    "            prune_index += 1\n",
    "            k += 1\n",
    "        \n",
    "        else:\n",
    "            print('Pruning Works!')\n",
    "            model = new_model\n",
    "            p -= 1\n",
    "    \n",
    "    print('finish training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.4830825924873352\n",
      "[ 1 ] | train_loss = 0.48308, train_acc = 0.79688, val_loss = 0.26112, val_acc = 0.84375\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.33521613851189613\n",
      "[ 1 ] | train_loss = 0.33522, train_acc = 0.85938, val_loss = 0.39031, val_acc = 0.84375\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.3010327561448018\n",
      "[ 1 ] | train_loss = 0.30103, train_acc = 0.87500, val_loss = 0.29390, val_acc = 0.88542\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.2653105690454443\n",
      "[ 1 ] | train_loss = 0.26531, train_acc = 0.89583, val_loss = 0.55169, val_acc = 0.87500\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.24319704823816815\n",
      "[ 1 ] | train_loss = 0.24320, train_acc = 0.90104, val_loss = 0.50937, val_acc = 0.87500\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.22292909615983567\n",
      "[ 1 ] | train_loss = 0.22293, train_acc = 0.92448, val_loss = 0.13421, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.19760205938170353\n",
      "[ 1 ] | train_loss = 0.19760, train_acc = 0.92969, val_loss = 0.18356, val_acc = 0.86458\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.20631467488904795\n",
      "[ 1 ] | train_loss = 0.20631, train_acc = 0.92708, val_loss = 0.49809, val_acc = 0.91667\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.18024826049804688\n",
      "[ 1 ] | train_loss = 0.18025, train_acc = 0.92448, val_loss = 0.18707, val_acc = 0.87500\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.19306819218521318\n",
      "[ 1 ] | train_loss = 0.19307, train_acc = 0.92969, val_loss = 0.30448, val_acc = 0.88542\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.22979157705170414\n",
      "[ 1 ] | train_loss = 0.22979, train_acc = 0.90885, val_loss = 0.69369, val_acc = 0.86458\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.2291433164694657\n",
      "[ 1 ] | train_loss = 0.22914, train_acc = 0.91406, val_loss = 0.47038, val_acc = 0.88542\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.19802, train_acc = 0.92969, val_loss = 0.62468, val_acc = 0.87500\n",
      "[ 2 ] | train_loss = 0.21378, train_acc = 0.91667, val_loss = 0.01582, val_acc = 0.91667\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.22710022795945406\n",
      "[ 1 ] | train_loss = 0.22710, train_acc = 0.90625, val_loss = 0.07004, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.12376, train_acc = 0.94792, val_loss = 0.04939, val_acc = 0.85417\n",
      "[ 2 ] | train_loss = 0.10453, train_acc = 0.96094, val_loss = 0.07927, val_acc = 0.88542\n",
      "[ 3 ] | train_loss = 0.25649, train_acc = 0.92448, val_loss = 0.26781, val_acc = 0.86458\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.1508249091760566\n",
      "[ 1 ] | train_loss = 0.15082, train_acc = 0.93750, val_loss = 0.05783, val_acc = 0.88542\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.08145221662319575\n",
      "[ 1 ] | train_loss = 0.08145, train_acc = 0.97135, val_loss = 0.03114, val_acc = 0.88542\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.07360, train_acc = 0.97396, val_loss = 0.04968, val_acc = 0.89583\n",
      "[ 2 ] | train_loss = 0.06921, train_acc = 0.98177, val_loss = 0.00873, val_acc = 0.89583\n",
      "[ 3 ] | train_loss = 0.19331, train_acc = 0.92188, val_loss = 0.04956, val_acc = 0.85417\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.08893431194398242\n",
      "[ 1 ] | train_loss = 0.08893, train_acc = 0.96094, val_loss = 0.00844, val_acc = 0.90625\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.05711749757756479\n",
      "[ 1 ] | train_loss = 0.05712, train_acc = 0.98177, val_loss = 0.04601, val_acc = 0.88542\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.05317, train_acc = 0.98177, val_loss = 0.09838, val_acc = 0.87500\n",
      "[ 2 ] | train_loss = 0.06402, train_acc = 0.98177, val_loss = 0.01277, val_acc = 0.90625\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.06291914267543082\n",
      "[ 1 ] | train_loss = 0.06292, train_acc = 0.96615, val_loss = 0.02031, val_acc = 0.88542\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.06855, train_acc = 0.97656, val_loss = 0.36251, val_acc = 0.85417\n",
      "[ 2 ] | train_loss = 0.11566, train_acc = 0.95573, val_loss = 0.06394, val_acc = 0.88542\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.08353658184447947\n",
      "[ 1 ] | train_loss = 0.08354, train_acc = 0.96354, val_loss = 0.01592, val_acc = 0.90625\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.09263, train_acc = 0.96615, val_loss = 0.22198, val_acc = 0.87500\n",
      "[ 2 ] | train_loss = 0.11474, train_acc = 0.96354, val_loss = 0.24108, val_acc = 0.89583\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.061600283050211146\n",
      "[ 1 ] | train_loss = 0.06160, train_acc = 0.96875, val_loss = 0.01654, val_acc = 0.92708\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.05736, train_acc = 0.98177, val_loss = 0.04102, val_acc = 0.86458\n",
      "[ 2 ] | train_loss = 0.04981, train_acc = 0.98438, val_loss = 0.00894, val_acc = 0.90625\n",
      "[ 3 ] | train_loss = 0.04658, train_acc = 0.98177, val_loss = 0.05663, val_acc = 0.89583\n",
      "[ 4 ] | train_loss = 0.04325, train_acc = 0.98438, val_loss = 0.04445, val_acc = 0.88542\n",
      "[ 5 ] | train_loss = 0.03006, train_acc = 0.98177, val_loss = 0.11322, val_acc = 0.90625\n",
      "[ 6 ] | train_loss = 0.03527, train_acc = 0.98177, val_loss = 0.01278, val_acc = 0.87500\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.021943278232356533\n",
      "[ 1 ] | train_loss = 0.02194, train_acc = 0.98958, val_loss = 0.00535, val_acc = 0.88542\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.019312258393862674\n",
      "[ 1 ] | train_loss = 0.01931, train_acc = 0.99740, val_loss = 0.00313, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.02210, train_acc = 0.98958, val_loss = 0.00065, val_acc = 0.91667\n",
      "[ 2 ] | train_loss = 0.02763, train_acc = 0.99219, val_loss = 0.00248, val_acc = 0.90625\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.02269196969427867\n",
      "[ 1 ] | train_loss = 0.02269, train_acc = 0.99219, val_loss = 0.00702, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.02065, train_acc = 0.99740, val_loss = 0.00483, val_acc = 0.89583\n",
      "[ 2 ] | train_loss = 0.04434, train_acc = 0.98438, val_loss = 0.05276, val_acc = 0.89583\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.03776118789877122\n",
      "[ 1 ] | train_loss = 0.03776, train_acc = 0.98698, val_loss = 0.07395, val_acc = 0.90625\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.03430, train_acc = 0.98438, val_loss = 0.08646, val_acc = 0.87500\n",
      "[ 2 ] | train_loss = 0.03746, train_acc = 0.98698, val_loss = 0.00861, val_acc = 0.88542\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.04242495367361698\n",
      "[ 1 ] | train_loss = 0.04242, train_acc = 0.98177, val_loss = 0.00101, val_acc = 0.87500\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.02835, train_acc = 0.98958, val_loss = 0.07152, val_acc = 0.88542\n",
      "[ 2 ] | train_loss = 0.02851, train_acc = 0.98698, val_loss = 0.02144, val_acc = 0.89583\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.02530172526167007\n",
      "[ 1 ] | train_loss = 0.02530, train_acc = 0.98698, val_loss = 0.17824, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.01822, train_acc = 0.99479, val_loss = 0.07483, val_acc = 0.89583\n",
      "[ 2 ] | train_loss = 0.01897, train_acc = 0.99479, val_loss = 0.01635, val_acc = 0.89583\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.017259097160907306\n",
      "[ 1 ] | train_loss = 0.01726, train_acc = 0.99740, val_loss = 0.00236, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.01725, train_acc = 0.99219, val_loss = 0.00571, val_acc = 0.89583\n",
      "[ 2 ] | train_loss = 0.01248, train_acc = 0.99740, val_loss = 0.01926, val_acc = 0.89583\n",
      "[ 3 ] | train_loss = 0.01555, train_acc = 0.99740, val_loss = 0.00280, val_acc = 0.88542\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.013620320856716717\n",
      "[ 1 ] | train_loss = 0.01362, train_acc = 0.99740, val_loss = 0.00182, val_acc = 0.90625\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.012522302276314198\n",
      "[ 1 ] | train_loss = 0.01252, train_acc = 0.99479, val_loss = 0.00039, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.01130, train_acc = 0.99740, val_loss = 0.00156, val_acc = 0.89583\n",
      "[ 2 ] | train_loss = 0.01434, train_acc = 0.99479, val_loss = 0.00014, val_acc = 0.89583\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.013803097827803867\n",
      "[ 1 ] | train_loss = 0.01380, train_acc = 0.99740, val_loss = 0.00300, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.00821, train_acc = 0.99740, val_loss = 0.00023, val_acc = 0.89583\n",
      "[ 2 ] | train_loss = 0.00845, train_acc = 1.00000, val_loss = 0.00674, val_acc = 0.89583\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.014726154309755657\n",
      "[ 1 ] | train_loss = 0.01473, train_acc = 0.99740, val_loss = 0.01630, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.00921, train_acc = 0.99740, val_loss = 0.00443, val_acc = 0.89583\n",
      "[ 2 ] | train_loss = 0.00867, train_acc = 1.00000, val_loss = 0.00150, val_acc = 0.89583\n",
      "[ 3 ] | train_loss = 0.02029, train_acc = 0.99479, val_loss = 0.00075, val_acc = 0.88542\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.01707433433944061\n",
      "[ 1 ] | train_loss = 0.01707, train_acc = 0.99219, val_loss = 0.00269, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.011186180790900835\n",
      "[ 1 ] | train_loss = 0.01119, train_acc = 0.99740, val_loss = 0.00297, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.01135, train_acc = 0.99740, val_loss = 0.14841, val_acc = 0.89583\n",
      "[ 2 ] | train_loss = 0.01855, train_acc = 0.99219, val_loss = 0.00316, val_acc = 0.90625\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.015014438802154473\n",
      "[ 1 ] | train_loss = 0.01501, train_acc = 0.99479, val_loss = 0.00233, val_acc = 0.89583\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.01816, train_acc = 0.99219, val_loss = 0.00103, val_acc = 0.88542\n",
      "[ 2 ] | train_loss = 0.01974, train_acc = 0.99219, val_loss = 0.01324, val_acc = 0.87500\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.009416552546554158\n",
      "[ 1 ] | train_loss = 0.00942, train_acc = 1.00000, val_loss = 0.03276, val_acc = 0.88542\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.04513, train_acc = 0.98958, val_loss = 0.01285, val_acc = 0.90625\n",
      "[ 2 ] | train_loss = 0.58160, train_acc = 0.91667, val_loss = 0.30104, val_acc = 0.78125\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.6546375300119204\n",
      "[ 1 ] | train_loss = 0.65464, train_acc = 0.92969, val_loss = 0.00876, val_acc = 0.83333\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.24486, train_acc = 0.93750, val_loss = 0.36451, val_acc = 0.84375\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.17487888754052014\n",
      "[ 1 ] | train_loss = 0.17488, train_acc = 0.95052, val_loss = 0.08965, val_acc = 0.88542\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.22967, train_acc = 0.95573, val_loss = 0.02929, val_acc = 0.89583\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.1422508580340794\n",
      "[ 1 ] | train_loss = 0.14225, train_acc = 0.94010, val_loss = 0.75417, val_acc = 0.86458\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.12022, train_acc = 0.96875, val_loss = 0.03020, val_acc = 0.86458\n",
      "[ 2 ] | train_loss = 0.34341, train_acc = 0.92448, val_loss = 0.96063, val_acc = 0.83333\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.042813663360751285\n",
      "[ 1 ] | train_loss = 0.04281, train_acc = 0.97917, val_loss = 0.00238, val_acc = 0.83333\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.02679, train_acc = 0.99219, val_loss = 0.01042, val_acc = 0.84375\n",
      "[ 2 ] | train_loss = 0.01865, train_acc = 0.99479, val_loss = 0.00486, val_acc = 0.84375\n",
      "[ 3 ] | train_loss = 0.01667, train_acc = 0.99479, val_loss = 0.01658, val_acc = 0.84375\n",
      "[ 4 ] | train_loss = 0.01143, train_acc = 0.99740, val_loss = 0.00624, val_acc = 0.84375\n",
      "[ 5 ] | train_loss = 0.01113, train_acc = 0.99740, val_loss = 0.00182, val_acc = 0.85417\n",
      "[ 6 ] | train_loss = 0.00880, train_acc = 0.99740, val_loss = 0.03364, val_acc = 0.85417\n",
      "[ 7 ] | train_loss = 0.00828, train_acc = 0.99740, val_loss = 0.00129, val_acc = 0.85417\n",
      "[ 8 ] | train_loss = 0.00796, train_acc = 0.99740, val_loss = 0.00867, val_acc = 0.85417\n",
      "[ 9 ] | train_loss = 0.00762, train_acc = 0.99740, val_loss = 0.00509, val_acc = 0.86458\n",
      "[ 10 ] | train_loss = 0.00660, train_acc = 0.99740, val_loss = 0.00047, val_acc = 0.86458\n",
      "[ 11 ] | train_loss = 0.00645, train_acc = 0.99740, val_loss = 0.00048, val_acc = 0.86458\n",
      "[ 12 ] | train_loss = 0.00529, train_acc = 0.99740, val_loss = 0.00013, val_acc = 0.86458\n",
      "[ 13 ] | train_loss = 0.00614, train_acc = 0.99740, val_loss = 0.00664, val_acc = 0.86458\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.004614084745109419\n",
      "[ 1 ] | train_loss = 0.00461, train_acc = 1.00000, val_loss = 0.00609, val_acc = 0.86458\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.00613, train_acc = 0.99740, val_loss = 0.00054, val_acc = 0.86458\n",
      "[ 2 ] | train_loss = 0.00442, train_acc = 0.99740, val_loss = 0.00018, val_acc = 0.86458\n",
      "[ 3 ] | train_loss = 0.00544, train_acc = 0.99740, val_loss = 0.00195, val_acc = 0.86458\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.004314945841656481\n",
      "[ 1 ] | train_loss = 0.00431, train_acc = 1.00000, val_loss = 0.00624, val_acc = 0.86458\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.004188373459574753\n",
      "[ 1 ] | train_loss = 0.00419, train_acc = 0.99740, val_loss = 0.00004, val_acc = 0.86458\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.00466, train_acc = 0.99740, val_loss = 0.00329, val_acc = 0.86458\n",
      "[ 2 ] | train_loss = 0.00429, train_acc = 1.00000, val_loss = 0.00607, val_acc = 0.86458\n",
      "[ 3 ] | train_loss = 0.00450, train_acc = 1.00000, val_loss = 0.00295, val_acc = 0.86458\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.0038150327076197734\n",
      "[ 1 ] | train_loss = 0.00382, train_acc = 1.00000, val_loss = 0.00866, val_acc = 0.86458\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "LG_UA_reg: Your eta_threshold is setting higher than your learning rate. Reset it with lower one!\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.003587595178032643\n",
      "[ 1 ] | train_loss = 0.00359, train_acc = 1.00000, val_loss = 0.00137, val_acc = 0.86458\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.00375, train_acc = 0.99740, val_loss = 0.00040, val_acc = 0.86458\n",
      "[ 2 ] | train_loss = 0.00386, train_acc = 1.00000, val_loss = 0.00080, val_acc = 0.86458\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.0035901813558515037\n",
      "[ 1 ] | train_loss = 0.00359, train_acc = 1.00000, val_loss = 0.00198, val_acc = 0.86458\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.00355, train_acc = 1.00000, val_loss = 0.00012, val_acc = 0.86458\n",
      "[ 2 ] | train_loss = 0.00420, train_acc = 1.00000, val_loss = 0.00144, val_acc = 0.86458\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.0031091797670039036\n",
      "[ 1 ] | train_loss = 0.00311, train_acc = 1.00000, val_loss = 0.00003, val_acc = 0.86458\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.00356, train_acc = 1.00000, val_loss = 0.00029, val_acc = 0.86458\n",
      "[ 2 ] | train_loss = 0.00332, train_acc = 1.00000, val_loss = 0.00580, val_acc = 0.86458\n",
      "[ 3 ] | train_loss = 0.00324, train_acc = 1.00000, val_loss = 0.00203, val_acc = 0.86458\n",
      "[ 4 ] | train_loss = 0.00311, train_acc = 1.00000, val_loss = 0.00384, val_acc = 0.86458\n",
      "[ 5 ] | train_loss = 0.00278, train_acc = 1.00000, val_loss = 0.00518, val_acc = 0.86458\n",
      "[ 6 ] | train_loss = 0.04420, train_acc = 0.99219, val_loss = 0.37674, val_acc = 0.83333\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.04287300695129185\n",
      "[ 1 ] | train_loss = 0.04287, train_acc = 0.98698, val_loss = 0.49445, val_acc = 0.87500\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.01998, train_acc = 0.99479, val_loss = 0.00229, val_acc = 0.85417\n",
      "[ 2 ] | train_loss = 0.06696, train_acc = 0.98698, val_loss = 0.00093, val_acc = 0.86458\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.0630511040195832\n",
      "[ 1 ] | train_loss = 0.06305, train_acc = 0.98698, val_loss = 0.00232, val_acc = 0.84375\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "[ 1 ] | train_loss = 0.01171, train_acc = 0.99479, val_loss = 0.00014, val_acc = 0.88542\n",
      "[ 2 ] | train_loss = 0.01948, train_acc = 0.99219, val_loss = 0.00483, val_acc = 0.88542\n",
      "LG_UA_reg: Restore previous model weights, stop training.\n",
      "Initialize weight-tuning LG UA model\n",
      "The previous training loss is: 0.015035219949557662\n",
      "[ 1 ] | train_loss = 0.01504, train_acc = 0.99479, val_loss = 0.00687, val_acc = 0.87500\n",
      "LG_UA_weight: The training loss is smaller than what you want, stop training. Acceptable\n",
      "Pruning Works!\n",
      "finish training\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model = BinaryClassification().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "All_r_LG_UA_w_LG_UA(train_loader, val_loader, model, criterion, optimizer, device, loss_threshold=0.9, eta_threshold=0.008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac3da779536756720bc930bbdcbe3b303a716c4190960bb8b007750e7b6b7c5d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
