{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath) -> None:\n",
    "        \n",
    "        # load csv data\n",
    "        data = pd.read_csv(filepath, header=None)\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "        \n",
    "        # feature scaling\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "        # convert to tensors\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('train_all_0.csv')\n",
    "\n",
    "# create data indices for train val split\n",
    "data_size = len(dataset)\n",
    "indices = list(range(data_size))\n",
    "split = int(np.floor(0.2 * data_size))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# create data loader\n",
    "train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=16, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self, dropout=False, batch_norm=False):\n",
    "        '''Once at a time'''\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(12, 128) \n",
    "        self.layer_out = nn.Linear(128, 2) \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if dropout:\n",
    "            self.dropout_1 = nn.Dropout(0.2)\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.batch_norm_1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if self.dropout:\n",
    "            x = self.relu(self.layer_1(inputs))\n",
    "            x = self.dropout_1(x)\n",
    "            x = self.layer_out(x)\n",
    "        \n",
    "        elif self.dropout:\n",
    "            x = self.relu(self.layer_1(inputs))\n",
    "            x = self.batch_norm_1(x)\n",
    "            x = self.layer_out(x)\n",
    "            \n",
    "        else:\n",
    "            x = self.relu(self.layer_1(inputs))\n",
    "            x = self.layer_out(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline based on page 47\n",
    "def LG_UA_regularization(train_loader=train_loader, val_loader=val_loader, model=None, criterion=None, optimizer=None, loss_threshold=0.5, eta_threshold=0.008, l2_lambda=0.001):\n",
    "    '''\n",
    "    Based on page 47, it should be L2 regularization and I can actually use \"weight_decay\" in pytorch optimizer.\n",
    "    But it's not fun so I will still implement L2 regularization by myself.\n",
    "    '''\n",
    "    previous_train_loss = 10000    \n",
    "\n",
    "    for epoch in itertools.count():\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        previous_model_params = model.state_dict()\n",
    "        stop_training = False\n",
    "        \n",
    "        while optimizer.param_groups[0]['lr'] > eta_threshold:\n",
    "            \n",
    "            train_loss = []\n",
    "            train_accs = []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                \n",
    "                x, y = batch\n",
    "                \n",
    "                logits = model(x.to(device))\n",
    "                loss = criterion(logits, y.to(device))\n",
    "                \n",
    "                # L2 regularization with normalized l2\n",
    "                L2_regularization = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                param_num = sum(p.numel() for p in model.parameters())\n",
    "                loss += (l2_lambda / param_num) * L2_regularization\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()\n",
    "                train_loss.append(loss.item())\n",
    "                train_accs.append(acc)\n",
    "            \n",
    "            max_train_loss = max(train_loss)\n",
    "            train_loss = sum(train_loss) / len(train_loss)\n",
    "            train_acc = sum(train_accs) / len(train_accs)\n",
    "            \n",
    "            \n",
    "            if train_loss < previous_train_loss:\n",
    "                if max_train_loss < loss_threshold:\n",
    "                    optimizer.param_groups[0]['lr'] *= 1.2\n",
    "                    previous_train_loss = train_loss\n",
    "                    # print(f'The previous training loss is: {previous_train_loss}')\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    model.load_state_dict(previous_model_params)\n",
    "                    stop_training = True\n",
    "                    # print(f'max loss: {max_train_loss} | loss_threshold: {loss_threshold}')\n",
    "                    break\n",
    "            \n",
    "            optimizer.param_groups[0]['lr'] *= 0.7\n",
    "            model.load_state_dict(previous_model_params)\n",
    "            # current_lr = optimizer.param_groups[0]['lr']\n",
    "            # print(f'lr shrinking!, now the lr is: {current_lr}')\n",
    "            \n",
    "        else:\n",
    "            stop_training = True\n",
    "            # print('learning rate < eta_threshold')\n",
    "        \n",
    "        # Use try and except to detect whether the eta_threshold is set too high initially\n",
    "        try:        \n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_accs = []\n",
    "            \n",
    "            for batch in val_loader:\n",
    "                imgs, labels = batch\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = model(imgs.to(device))\n",
    "                    \n",
    "                    acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "                    valid_loss.append(loss.item())\n",
    "                    valid_accs.append(acc)\n",
    "            \n",
    "            valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "            valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "        \n",
    "            print(f'[ {epoch+1} ] | train_loss = {train_loss:.5f}, train_acc = {train_acc:.5f}, val_loss = {valid_loss:.5f}, val_acc = {valid_acc:.5f}')\n",
    "            \n",
    "        except UnboundLocalError:\n",
    "            print('Your eta_threshold is setting higher than your learning rate. Reset it with lower one!')\n",
    "        \n",
    "        # stopping criterion\n",
    "        if stop_training:\n",
    "            print('Restore previous model weights, stop training.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 ] | train_loss = 0.56748, train_acc = 0.72656, val_loss = 0.33394, val_acc = 0.76042\n",
      "[ 2 ] | train_loss = 0.44250, train_acc = 0.82031, val_loss = 0.31409, val_acc = 0.79167\n",
      "[ 3 ] | train_loss = 0.37139, train_acc = 0.86198, val_loss = 0.19311, val_acc = 0.80208\n",
      "[ 4 ] | train_loss = 0.33307, train_acc = 0.86458, val_loss = 0.30488, val_acc = 0.83333\n",
      "[ 5 ] | train_loss = 0.30860, train_acc = 0.88802, val_loss = 0.16566, val_acc = 0.82292\n",
      "[ 6 ] | train_loss = 0.29882, train_acc = 0.88281, val_loss = 0.28512, val_acc = 0.86458\n",
      "[ 7 ] | train_loss = 0.27554, train_acc = 0.89062, val_loss = 0.22133, val_acc = 0.85417\n",
      "[ 8 ] | train_loss = 0.26412, train_acc = 0.90625, val_loss = 0.24282, val_acc = 0.84375\n",
      "[ 9 ] | train_loss = 0.24601, train_acc = 0.90885, val_loss = 0.19254, val_acc = 0.89583\n",
      "[ 10 ] | train_loss = 0.23799, train_acc = 0.91406, val_loss = 0.06659, val_acc = 0.89583\n",
      "[ 11 ] | train_loss = 0.23598, train_acc = 0.90625, val_loss = 0.25251, val_acc = 0.87500\n",
      "[ 12 ] | train_loss = 0.21445, train_acc = 0.91927, val_loss = 0.08332, val_acc = 0.91667\n",
      "[ 13 ] | train_loss = 0.20738, train_acc = 0.91406, val_loss = 0.34310, val_acc = 0.91667\n",
      "[ 14 ] | train_loss = 0.17707, train_acc = 0.93229, val_loss = 0.10949, val_acc = 0.92708\n",
      "[ 15 ] | train_loss = 0.15922, train_acc = 0.94010, val_loss = 0.13087, val_acc = 0.93750\n",
      "[ 16 ] | train_loss = 0.14963, train_acc = 0.94531, val_loss = 0.19270, val_acc = 0.93750\n",
      "[ 17 ] | train_loss = 0.13359, train_acc = 0.95312, val_loss = 0.05941, val_acc = 0.93750\n",
      "[ 18 ] | train_loss = 0.12784, train_acc = 0.95312, val_loss = 0.07531, val_acc = 0.95833\n",
      "[ 19 ] | train_loss = 0.10964, train_acc = 0.96094, val_loss = 0.05484, val_acc = 0.93750\n",
      "[ 20 ] | train_loss = 0.09056, train_acc = 0.96875, val_loss = 0.03173, val_acc = 0.94792\n",
      "[ 21 ] | train_loss = 0.08525, train_acc = 0.96615, val_loss = 0.03530, val_acc = 0.93750\n",
      "[ 22 ] | train_loss = 0.07568, train_acc = 0.97656, val_loss = 0.05365, val_acc = 0.92708\n",
      "[ 23 ] | train_loss = 0.07126, train_acc = 0.97917, val_loss = 0.04719, val_acc = 0.94792\n",
      "[ 24 ] | train_loss = 0.06764, train_acc = 0.97656, val_loss = 0.05083, val_acc = 0.93750\n",
      "[ 25 ] | train_loss = 0.06157, train_acc = 0.98177, val_loss = 0.18140, val_acc = 0.95833\n",
      "[ 26 ] | train_loss = 0.05580, train_acc = 0.98438, val_loss = 0.05897, val_acc = 0.95833\n",
      "[ 27 ] | train_loss = 0.05040, train_acc = 0.98698, val_loss = 0.03464, val_acc = 0.95833\n",
      "[ 28 ] | train_loss = 0.04999, train_acc = 0.98438, val_loss = 0.02482, val_acc = 0.94792\n",
      "[ 29 ] | train_loss = 0.04672, train_acc = 0.98958, val_loss = 0.01574, val_acc = 0.95833\n",
      "[ 30 ] | train_loss = 0.04488, train_acc = 0.98958, val_loss = 0.08481, val_acc = 0.95833\n",
      "[ 31 ] | train_loss = 0.04409, train_acc = 0.98698, val_loss = 0.03185, val_acc = 0.95833\n",
      "[ 32 ] | train_loss = 0.04390, train_acc = 0.98958, val_loss = 0.08087, val_acc = 0.95833\n",
      "[ 33 ] | train_loss = 0.04231, train_acc = 0.98698, val_loss = 0.00953, val_acc = 0.94792\n",
      "[ 34 ] | train_loss = 0.04045, train_acc = 0.98958, val_loss = 0.01659, val_acc = 0.95833\n",
      "[ 35 ] | train_loss = 0.04004, train_acc = 0.98698, val_loss = 0.05371, val_acc = 0.95833\n",
      "[ 36 ] | train_loss = 0.03788, train_acc = 0.98958, val_loss = 0.04584, val_acc = 0.95833\n",
      "[ 37 ] | train_loss = 0.03690, train_acc = 0.99219, val_loss = 0.00539, val_acc = 0.95833\n",
      "[ 38 ] | train_loss = 0.03639, train_acc = 0.99219, val_loss = 0.02177, val_acc = 0.95833\n",
      "[ 39 ] | train_loss = 0.03504, train_acc = 0.98958, val_loss = 0.03593, val_acc = 0.95833\n",
      "[ 40 ] | train_loss = 0.03478, train_acc = 0.99219, val_loss = 0.01965, val_acc = 0.95833\n",
      "[ 41 ] | train_loss = 0.03421, train_acc = 0.98958, val_loss = 0.04806, val_acc = 0.95833\n",
      "[ 42 ] | train_loss = 0.03409, train_acc = 0.99219, val_loss = 0.04112, val_acc = 0.95833\n",
      "[ 43 ] | train_loss = 0.03394, train_acc = 0.98958, val_loss = 0.03887, val_acc = 0.95833\n",
      "[ 44 ] | train_loss = 0.03258, train_acc = 0.99219, val_loss = 0.05217, val_acc = 0.95833\n",
      "[ 45 ] | train_loss = 0.03213, train_acc = 0.99219, val_loss = 0.04605, val_acc = 0.95833\n",
      "[ 46 ] | train_loss = 0.03192, train_acc = 0.99219, val_loss = 0.04876, val_acc = 0.95833\n",
      "[ 47 ] | train_loss = 0.03159, train_acc = 0.99219, val_loss = 0.00610, val_acc = 0.95833\n",
      "[ 48 ] | train_loss = 0.03148, train_acc = 0.99219, val_loss = 0.02690, val_acc = 0.95833\n",
      "[ 49 ] | train_loss = 0.03003, train_acc = 0.99219, val_loss = 0.00591, val_acc = 0.95833\n",
      "[ 50 ] | train_loss = 0.02997, train_acc = 0.99219, val_loss = 0.02054, val_acc = 0.95833\n",
      "[ 51 ] | train_loss = 0.02958, train_acc = 0.99219, val_loss = 0.08657, val_acc = 0.95833\n",
      "[ 52 ] | train_loss = 0.02923, train_acc = 0.99219, val_loss = 0.01889, val_acc = 0.95833\n",
      "[ 53 ] | train_loss = 0.02910, train_acc = 0.99219, val_loss = 0.00819, val_acc = 0.95833\n",
      "[ 54 ] | train_loss = 0.02908, train_acc = 0.99219, val_loss = 0.02718, val_acc = 0.95833\n",
      "[ 55 ] | train_loss = 0.02907, train_acc = 0.99219, val_loss = 0.00817, val_acc = 0.95833\n",
      "[ 56 ] | train_loss = 0.02903, train_acc = 0.99219, val_loss = 0.03721, val_acc = 0.95833\n",
      "[ 57 ] | train_loss = 0.02863, train_acc = 0.99219, val_loss = 0.05187, val_acc = 0.95833\n",
      "[ 58 ] | train_loss = 0.02860, train_acc = 0.99219, val_loss = 0.03198, val_acc = 0.95833\n",
      "[ 59 ] | train_loss = 0.02857, train_acc = 0.99479, val_loss = 0.01066, val_acc = 0.95833\n",
      "[ 60 ] | train_loss = 0.02819, train_acc = 0.99219, val_loss = 0.01453, val_acc = 0.95833\n",
      "[ 61 ] | train_loss = 0.02819, train_acc = 0.99219, val_loss = 0.02376, val_acc = 0.95833\n",
      "[ 62 ] | train_loss = 0.02805, train_acc = 0.99219, val_loss = 0.01134, val_acc = 0.95833\n",
      "[ 63 ] | train_loss = 0.02793, train_acc = 0.99219, val_loss = 0.02354, val_acc = 0.95833\n",
      "[ 64 ] | train_loss = 0.02776, train_acc = 0.99219, val_loss = 0.08566, val_acc = 0.95833\n",
      "[ 65 ] | train_loss = 0.02763, train_acc = 0.99219, val_loss = 0.05401, val_acc = 0.95833\n",
      "[ 66 ] | train_loss = 0.02759, train_acc = 0.99219, val_loss = 0.01038, val_acc = 0.95833\n",
      "[ 67 ] | train_loss = 0.02756, train_acc = 0.99219, val_loss = 0.02222, val_acc = 0.95833\n",
      "[ 68 ] | train_loss = 0.02750, train_acc = 0.99219, val_loss = 0.02180, val_acc = 0.95833\n",
      "[ 69 ] | train_loss = 0.02732, train_acc = 0.99219, val_loss = 0.03369, val_acc = 0.95833\n",
      "[ 70 ] | train_loss = 0.02741, train_acc = 0.99219, val_loss = 0.00282, val_acc = 0.95833\n",
      "Restore previous model weights, stop training.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BinaryClassification().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "LG_UA_regularization(train_loader, val_loader, model, criterion, optimizer, loss_threshold=0.9, eta_threshold=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline based on page 48\n",
    "def EU_LG_UA_regularization(train_loader=train_loader, val_loader=val_loader, model=None, epochs=1, criterion=None, optimizer=None, loss_threshold=0.5, eta_threshold=0.008, l2_lambda=0.001):\n",
    "    '''\n",
    "    Based on page 47, it should be L2 regularization and I can actually use \"weight_decay\" in pytorch optimizer.\n",
    "    But it's not fun so I will still implement L2 regularization by myself.\n",
    "    '''\n",
    "    previous_train_loss = 10000    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        previous_model_params = model.state_dict()\n",
    "        stop_training = False\n",
    "        \n",
    "        while optimizer.param_groups[0]['lr'] > eta_threshold:\n",
    "            \n",
    "            train_loss = []\n",
    "            train_accs = []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                \n",
    "                x, y = batch\n",
    "                \n",
    "                logits = model(x.to(device))\n",
    "                loss = criterion(logits, y.to(device))\n",
    "                \n",
    "                # L2 regularization with normalized l2\n",
    "                L2_regularization = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                param_num = sum(p.numel() for p in model.parameters())\n",
    "                loss += (l2_lambda / param_num) * L2_regularization\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()\n",
    "                train_loss.append(loss.item())\n",
    "                train_accs.append(acc)\n",
    "            \n",
    "            max_train_loss = max(train_loss)\n",
    "            train_loss = sum(train_loss) / len(train_loss)\n",
    "            train_acc = sum(train_accs) / len(train_accs)\n",
    "            \n",
    "            \n",
    "            if train_loss < previous_train_loss:\n",
    "                if max_train_loss < loss_threshold:\n",
    "                    optimizer.param_groups[0]['lr'] *= 1.2\n",
    "                    previous_train_loss = train_loss\n",
    "                    # print(f'The previous training loss is: {previous_train_loss}')\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    model.load_state_dict(previous_model_params)\n",
    "                    stop_training = True\n",
    "                    # print(f'max loss: {max_train_loss} | loss_threshold: {loss_threshold}')\n",
    "                    break\n",
    "            \n",
    "            optimizer.param_groups[0]['lr'] *= 0.7\n",
    "            model.load_state_dict(previous_model_params)\n",
    "            # current_lr = optimizer.param_groups[0]['lr']\n",
    "            # print(f'lr shrinking!, now the lr is: {current_lr}')\n",
    "            \n",
    "        else:\n",
    "            stop_training = True\n",
    "            # print('learning rate < eta_threshold')\n",
    "        \n",
    "        # Use try and except to detect whether the eta_threshold is set too high initially\n",
    "        try:        \n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_accs = []\n",
    "            \n",
    "            for batch in val_loader:\n",
    "                imgs, labels = batch\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = model(imgs.to(device))\n",
    "                    \n",
    "                    acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "                    valid_loss.append(loss.item())\n",
    "                    valid_accs.append(acc)\n",
    "            \n",
    "            valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "            valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "        \n",
    "            print(f'[ {epoch+1}/{epochs} ] | train_loss = {train_loss:.5f}, train_acc = {train_acc:.5f}, val_loss = {valid_loss:.5f}, val_acc = {valid_acc:.5f}')\n",
    "            \n",
    "        except UnboundLocalError:\n",
    "            print('Your eta_threshold is setting higher than your learning rate. Reset it with lower one!')\n",
    "        \n",
    "        # stopping criterion\n",
    "        if stop_training:\n",
    "            print('Restore previous model weights, stop training.')\n",
    "            break\n",
    "        \n",
    "        if epoch+1 >= 50:\n",
    "            print(\"It's over 50 epochs, stop training\")    \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100 ] | train_loss = 0.56600, train_acc = 0.79167, val_loss = 0.49526, val_acc = 0.76042\n",
      "[ 2/100 ] | train_loss = 0.43306, train_acc = 0.82812, val_loss = 0.42078, val_acc = 0.79167\n",
      "[ 3/100 ] | train_loss = 0.36971, train_acc = 0.86198, val_loss = 0.19177, val_acc = 0.80208\n",
      "[ 4/100 ] | train_loss = 0.33529, train_acc = 0.86198, val_loss = 0.39725, val_acc = 0.81250\n",
      "[ 5/100 ] | train_loss = 0.31347, train_acc = 0.88021, val_loss = 0.22493, val_acc = 0.84375\n",
      "[ 6/100 ] | train_loss = 0.29172, train_acc = 0.88281, val_loss = 0.14767, val_acc = 0.86458\n",
      "[ 7/100 ] | train_loss = 0.27862, train_acc = 0.90104, val_loss = 0.59157, val_acc = 0.86458\n",
      "[ 8/100 ] | train_loss = 0.26701, train_acc = 0.89323, val_loss = 0.36960, val_acc = 0.88542\n",
      "[ 9/100 ] | train_loss = 0.25352, train_acc = 0.89844, val_loss = 0.20255, val_acc = 0.87500\n",
      "[ 10/100 ] | train_loss = 0.23497, train_acc = 0.91146, val_loss = 0.24970, val_acc = 0.89583\n",
      "[ 11/100 ] | train_loss = 0.22420, train_acc = 0.92188, val_loss = 0.45684, val_acc = 0.88542\n",
      "[ 12/100 ] | train_loss = 0.21661, train_acc = 0.92969, val_loss = 0.18683, val_acc = 0.89583\n",
      "[ 13/100 ] | train_loss = 0.21146, train_acc = 0.92188, val_loss = 0.10352, val_acc = 0.93750\n",
      "[ 14/100 ] | train_loss = 0.20564, train_acc = 0.91927, val_loss = 0.12722, val_acc = 0.87500\n",
      "[ 15/100 ] | train_loss = 0.16936, train_acc = 0.92969, val_loss = 0.13493, val_acc = 0.93750\n",
      "[ 16/100 ] | train_loss = 0.14659, train_acc = 0.94010, val_loss = 0.03750, val_acc = 0.94792\n",
      "[ 17/100 ] | train_loss = 0.13919, train_acc = 0.94531, val_loss = 0.16366, val_acc = 0.94792\n",
      "[ 18/100 ] | train_loss = 0.13029, train_acc = 0.94010, val_loss = 0.04455, val_acc = 0.95833\n",
      "[ 19/100 ] | train_loss = 0.11247, train_acc = 0.95573, val_loss = 0.32350, val_acc = 0.92708\n",
      "[ 20/100 ] | train_loss = 0.10338, train_acc = 0.95312, val_loss = 0.08683, val_acc = 0.94792\n",
      "[ 21/100 ] | train_loss = 0.10053, train_acc = 0.95052, val_loss = 0.02906, val_acc = 0.96875\n",
      "[ 22/100 ] | train_loss = 0.08173, train_acc = 0.97396, val_loss = 0.02713, val_acc = 0.94792\n",
      "[ 23/100 ] | train_loss = 0.07669, train_acc = 0.97917, val_loss = 0.03834, val_acc = 0.94792\n",
      "[ 24/100 ] | train_loss = 0.07382, train_acc = 0.96615, val_loss = 0.10040, val_acc = 0.91667\n",
      "[ 25/100 ] | train_loss = 0.05678, train_acc = 0.98438, val_loss = 0.10594, val_acc = 0.95833\n",
      "[ 26/100 ] | train_loss = 0.05424, train_acc = 0.98698, val_loss = 0.02512, val_acc = 0.95833\n",
      "[ 27/100 ] | train_loss = 0.05155, train_acc = 0.98177, val_loss = 0.03341, val_acc = 0.96875\n",
      "[ 28/100 ] | train_loss = 0.04950, train_acc = 0.98698, val_loss = 0.03077, val_acc = 0.96875\n",
      "[ 29/100 ] | train_loss = 0.04707, train_acc = 0.98438, val_loss = 0.03011, val_acc = 0.95833\n",
      "[ 30/100 ] | train_loss = 0.04393, train_acc = 0.98698, val_loss = 0.04068, val_acc = 0.96875\n",
      "[ 31/100 ] | train_loss = 0.04336, train_acc = 0.98958, val_loss = 0.06503, val_acc = 0.95833\n",
      "[ 32/100 ] | train_loss = 0.03752, train_acc = 0.98958, val_loss = 0.02066, val_acc = 0.95833\n",
      "[ 33/100 ] | train_loss = 0.03488, train_acc = 0.99479, val_loss = 0.03237, val_acc = 0.96875\n",
      "[ 34/100 ] | train_loss = 0.03431, train_acc = 0.99479, val_loss = 0.01273, val_acc = 0.95833\n",
      "[ 35/100 ] | train_loss = 0.03375, train_acc = 0.99219, val_loss = 0.04939, val_acc = 0.95833\n",
      "[ 36/100 ] | train_loss = 0.03320, train_acc = 0.99219, val_loss = 0.00537, val_acc = 0.96875\n",
      "[ 37/100 ] | train_loss = 0.03278, train_acc = 0.99479, val_loss = 0.00609, val_acc = 0.96875\n",
      "[ 38/100 ] | train_loss = 0.03232, train_acc = 0.99479, val_loss = 0.01772, val_acc = 0.95833\n",
      "[ 39/100 ] | train_loss = 0.03174, train_acc = 0.99479, val_loss = 0.05169, val_acc = 0.96875\n",
      "[ 40/100 ] | train_loss = 0.03156, train_acc = 0.99479, val_loss = 0.01045, val_acc = 0.96875\n",
      "[ 41/100 ] | train_loss = 0.03124, train_acc = 0.99479, val_loss = 0.04872, val_acc = 0.95833\n",
      "[ 42/100 ] | train_loss = 0.03095, train_acc = 0.99479, val_loss = 0.00784, val_acc = 0.96875\n",
      "[ 43/100 ] | train_loss = 0.03008, train_acc = 0.99479, val_loss = 0.04496, val_acc = 0.96875\n",
      "[ 44/100 ] | train_loss = 0.03001, train_acc = 0.99479, val_loss = 0.05471, val_acc = 0.95833\n",
      "[ 45/100 ] | train_loss = 0.02981, train_acc = 0.99479, val_loss = 0.02214, val_acc = 0.96875\n",
      "[ 46/100 ] | train_loss = 0.02924, train_acc = 0.99479, val_loss = 0.11280, val_acc = 0.96875\n",
      "[ 47/100 ] | train_loss = 0.02919, train_acc = 0.99479, val_loss = 0.02520, val_acc = 0.96875\n",
      "[ 48/100 ] | train_loss = 0.02881, train_acc = 0.99479, val_loss = 0.01645, val_acc = 0.95833\n",
      "[ 49/100 ] | train_loss = 0.02856, train_acc = 0.99479, val_loss = 0.03253, val_acc = 0.96875\n",
      "[ 50/100 ] | train_loss = 0.02849, train_acc = 0.99479, val_loss = 0.00361, val_acc = 0.96875\n",
      "It's over 50 epochs, stop training\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BinaryClassification().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "EU_LG_UA_regularization(train_loader, val_loader, model, epochs=100, criterion=criterion, optimizer=optimizer, loss_threshold=0.9, eta_threshold=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline\n",
    "def regularization(train_loader=train_loader, val_loader=val_loader, model=None, epochs=None, criterion=None, optimizer=None, l2_lambda=0.001):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            \n",
    "            x, y = batch\n",
    "            \n",
    "            logits = model(x.to(device))\n",
    "            loss = criterion(logits, y.to(device))\n",
    "            \n",
    "            # L2 regularization with normalized l2\n",
    "            L2_regularization = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            param_num = sum(p.numel() for p in model.parameters())\n",
    "            loss += (l2_lambda / param_num) * L2_regularization\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()\n",
    "            train_loss.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "        \n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        train_acc = sum(train_accs) / len(train_accs)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        valid_loss = []\n",
    "        valid_accs = []\n",
    "        \n",
    "        for batch in val_loader:\n",
    "            imgs, labels = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model(imgs.to(device))\n",
    "                \n",
    "                acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "                \n",
    "                valid_loss.append(loss.item())\n",
    "                valid_accs.append(acc)\n",
    "        \n",
    "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "        \n",
    "        print(f'[ {epoch+1}/{epochs} ] | train_loss = {train_loss:.5f}, train_acc = {train_acc:.5f}, val_loss = {valid_loss:.5f}, val_acc = {valid_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100 ] | train_loss = 0.66423, train_acc = 0.55208, val_loss = 0.61592, val_acc = 0.76042\n",
      "[ 2/100 ] | train_loss = 0.48022, train_acc = 0.83333, val_loss = 0.61011, val_acc = 0.78125\n",
      "[ 3/100 ] | train_loss = 0.40645, train_acc = 0.86198, val_loss = 0.43490, val_acc = 0.80208\n",
      "[ 4/100 ] | train_loss = 0.36450, train_acc = 0.85417, val_loss = 0.34818, val_acc = 0.81250\n",
      "[ 5/100 ] | train_loss = 0.34894, train_acc = 0.86198, val_loss = 0.21656, val_acc = 0.80208\n",
      "[ 6/100 ] | train_loss = 0.32327, train_acc = 0.86719, val_loss = 0.43926, val_acc = 0.81250\n",
      "[ 7/100 ] | train_loss = 0.32068, train_acc = 0.87240, val_loss = 0.32941, val_acc = 0.84375\n",
      "[ 8/100 ] | train_loss = 0.31098, train_acc = 0.87500, val_loss = 0.18890, val_acc = 0.84375\n",
      "[ 9/100 ] | train_loss = 0.30062, train_acc = 0.87760, val_loss = 0.33761, val_acc = 0.85417\n",
      "[ 10/100 ] | train_loss = 0.29693, train_acc = 0.89062, val_loss = 0.43440, val_acc = 0.86458\n",
      "[ 11/100 ] | train_loss = 0.28962, train_acc = 0.89583, val_loss = 0.51376, val_acc = 0.84375\n",
      "[ 12/100 ] | train_loss = 0.28967, train_acc = 0.88281, val_loss = 0.24526, val_acc = 0.86458\n",
      "[ 13/100 ] | train_loss = 0.28615, train_acc = 0.89323, val_loss = 0.35914, val_acc = 0.86458\n",
      "[ 14/100 ] | train_loss = 0.27490, train_acc = 0.90104, val_loss = 0.12059, val_acc = 0.86458\n",
      "[ 15/100 ] | train_loss = 0.26778, train_acc = 0.90104, val_loss = 0.24941, val_acc = 0.87500\n",
      "[ 16/100 ] | train_loss = 0.25845, train_acc = 0.90625, val_loss = 0.13238, val_acc = 0.86458\n",
      "[ 17/100 ] | train_loss = 0.25491, train_acc = 0.90365, val_loss = 0.12126, val_acc = 0.86458\n",
      "[ 18/100 ] | train_loss = 0.25268, train_acc = 0.88802, val_loss = 0.12887, val_acc = 0.86458\n",
      "[ 19/100 ] | train_loss = 0.25139, train_acc = 0.90104, val_loss = 0.24903, val_acc = 0.87500\n",
      "[ 20/100 ] | train_loss = 0.24956, train_acc = 0.90365, val_loss = 0.16405, val_acc = 0.86458\n",
      "[ 21/100 ] | train_loss = 0.25178, train_acc = 0.90365, val_loss = 0.07027, val_acc = 0.88542\n",
      "[ 22/100 ] | train_loss = 0.23480, train_acc = 0.91146, val_loss = 0.32318, val_acc = 0.88542\n",
      "[ 23/100 ] | train_loss = 0.23706, train_acc = 0.91406, val_loss = 0.36634, val_acc = 0.89583\n",
      "[ 24/100 ] | train_loss = 0.23440, train_acc = 0.90885, val_loss = 0.16677, val_acc = 0.88542\n",
      "[ 25/100 ] | train_loss = 0.22705, train_acc = 0.91146, val_loss = 0.14130, val_acc = 0.88542\n",
      "[ 26/100 ] | train_loss = 0.22462, train_acc = 0.91927, val_loss = 0.11476, val_acc = 0.89583\n",
      "[ 27/100 ] | train_loss = 0.23206, train_acc = 0.90885, val_loss = 0.48522, val_acc = 0.87500\n",
      "[ 28/100 ] | train_loss = 0.21696, train_acc = 0.92188, val_loss = 0.56924, val_acc = 0.89583\n",
      "[ 29/100 ] | train_loss = 0.21878, train_acc = 0.91927, val_loss = 0.10333, val_acc = 0.89583\n",
      "[ 30/100 ] | train_loss = 0.22040, train_acc = 0.91146, val_loss = 0.11331, val_acc = 0.88542\n",
      "[ 31/100 ] | train_loss = 0.21740, train_acc = 0.91667, val_loss = 0.41524, val_acc = 0.89583\n",
      "[ 32/100 ] | train_loss = 0.21148, train_acc = 0.92708, val_loss = 0.21651, val_acc = 0.89583\n",
      "[ 33/100 ] | train_loss = 0.21028, train_acc = 0.90625, val_loss = 0.25035, val_acc = 0.88542\n",
      "[ 34/100 ] | train_loss = 0.20476, train_acc = 0.91927, val_loss = 0.18808, val_acc = 0.89583\n",
      "[ 35/100 ] | train_loss = 0.20340, train_acc = 0.92708, val_loss = 0.40938, val_acc = 0.89583\n",
      "[ 36/100 ] | train_loss = 0.19647, train_acc = 0.92969, val_loss = 0.20420, val_acc = 0.89583\n",
      "[ 37/100 ] | train_loss = 0.19794, train_acc = 0.93490, val_loss = 0.39090, val_acc = 0.89583\n",
      "[ 38/100 ] | train_loss = 0.19848, train_acc = 0.92708, val_loss = 0.12012, val_acc = 0.89583\n",
      "[ 39/100 ] | train_loss = 0.19522, train_acc = 0.91667, val_loss = 0.18619, val_acc = 0.88542\n",
      "[ 40/100 ] | train_loss = 0.19590, train_acc = 0.92448, val_loss = 0.09702, val_acc = 0.90625\n",
      "[ 41/100 ] | train_loss = 0.20293, train_acc = 0.91927, val_loss = 0.39624, val_acc = 0.87500\n",
      "[ 42/100 ] | train_loss = 0.19983, train_acc = 0.92448, val_loss = 0.41881, val_acc = 0.89583\n",
      "[ 43/100 ] | train_loss = 0.18356, train_acc = 0.93750, val_loss = 0.14228, val_acc = 0.90625\n",
      "[ 44/100 ] | train_loss = 0.19683, train_acc = 0.92969, val_loss = 0.08794, val_acc = 0.90625\n",
      "[ 45/100 ] | train_loss = 0.18665, train_acc = 0.93229, val_loss = 0.04741, val_acc = 0.89583\n",
      "[ 46/100 ] | train_loss = 0.19933, train_acc = 0.91927, val_loss = 0.12906, val_acc = 0.88542\n",
      "[ 47/100 ] | train_loss = 0.18830, train_acc = 0.92969, val_loss = 0.14975, val_acc = 0.88542\n",
      "[ 48/100 ] | train_loss = 0.17941, train_acc = 0.93490, val_loss = 0.19606, val_acc = 0.89583\n",
      "[ 49/100 ] | train_loss = 0.18171, train_acc = 0.92708, val_loss = 0.11837, val_acc = 0.92708\n",
      "[ 50/100 ] | train_loss = 0.17641, train_acc = 0.92708, val_loss = 0.04753, val_acc = 0.88542\n",
      "[ 51/100 ] | train_loss = 0.17045, train_acc = 0.92708, val_loss = 0.23275, val_acc = 0.88542\n",
      "[ 52/100 ] | train_loss = 0.17476, train_acc = 0.92708, val_loss = 0.65119, val_acc = 0.90625\n",
      "[ 53/100 ] | train_loss = 0.18061, train_acc = 0.93229, val_loss = 0.07614, val_acc = 0.89583\n",
      "[ 54/100 ] | train_loss = 0.16335, train_acc = 0.94271, val_loss = 0.07407, val_acc = 0.92708\n",
      "[ 55/100 ] | train_loss = 0.17192, train_acc = 0.94010, val_loss = 0.12313, val_acc = 0.90625\n",
      "[ 56/100 ] | train_loss = 0.17323, train_acc = 0.93490, val_loss = 0.31892, val_acc = 0.89583\n",
      "[ 57/100 ] | train_loss = 0.17627, train_acc = 0.92708, val_loss = 0.08169, val_acc = 0.92708\n",
      "[ 58/100 ] | train_loss = 0.17344, train_acc = 0.92188, val_loss = 0.23301, val_acc = 0.89583\n",
      "[ 59/100 ] | train_loss = 0.16758, train_acc = 0.94010, val_loss = 0.18424, val_acc = 0.91667\n",
      "[ 60/100 ] | train_loss = 0.15840, train_acc = 0.93229, val_loss = 0.16273, val_acc = 0.90625\n",
      "[ 61/100 ] | train_loss = 0.15259, train_acc = 0.93750, val_loss = 0.05260, val_acc = 0.91667\n",
      "[ 62/100 ] | train_loss = 0.14766, train_acc = 0.92188, val_loss = 0.09538, val_acc = 0.92708\n",
      "[ 63/100 ] | train_loss = 0.15605, train_acc = 0.93229, val_loss = 0.20485, val_acc = 0.90625\n",
      "[ 64/100 ] | train_loss = 0.16463, train_acc = 0.93229, val_loss = 0.04798, val_acc = 0.91667\n",
      "[ 65/100 ] | train_loss = 0.16085, train_acc = 0.92969, val_loss = 0.24087, val_acc = 0.91667\n",
      "[ 66/100 ] | train_loss = 0.14408, train_acc = 0.94271, val_loss = 0.21741, val_acc = 0.90625\n",
      "[ 67/100 ] | train_loss = 0.14729, train_acc = 0.94531, val_loss = 0.04228, val_acc = 0.90625\n",
      "[ 68/100 ] | train_loss = 0.15810, train_acc = 0.93229, val_loss = 0.10348, val_acc = 0.92708\n",
      "[ 69/100 ] | train_loss = 0.13561, train_acc = 0.95573, val_loss = 0.10190, val_acc = 0.90625\n",
      "[ 70/100 ] | train_loss = 0.13888, train_acc = 0.94531, val_loss = 0.06190, val_acc = 0.91667\n",
      "[ 71/100 ] | train_loss = 0.13809, train_acc = 0.93750, val_loss = 0.10253, val_acc = 0.92708\n",
      "[ 72/100 ] | train_loss = 0.14172, train_acc = 0.95052, val_loss = 0.19126, val_acc = 0.91667\n",
      "[ 73/100 ] | train_loss = 0.15855, train_acc = 0.94271, val_loss = 0.05146, val_acc = 0.91667\n",
      "[ 74/100 ] | train_loss = 0.14608, train_acc = 0.95052, val_loss = 0.11154, val_acc = 0.92708\n",
      "[ 75/100 ] | train_loss = 0.14507, train_acc = 0.95052, val_loss = 0.23167, val_acc = 0.92708\n",
      "[ 76/100 ] | train_loss = 0.13652, train_acc = 0.94531, val_loss = 0.14574, val_acc = 0.91667\n",
      "[ 77/100 ] | train_loss = 0.14379, train_acc = 0.95052, val_loss = 0.27549, val_acc = 0.91667\n",
      "[ 78/100 ] | train_loss = 0.13831, train_acc = 0.94792, val_loss = 0.04369, val_acc = 0.91667\n",
      "[ 79/100 ] | train_loss = 0.13828, train_acc = 0.93750, val_loss = 0.09334, val_acc = 0.92708\n",
      "[ 80/100 ] | train_loss = 0.13843, train_acc = 0.94010, val_loss = 0.15764, val_acc = 0.92708\n",
      "[ 81/100 ] | train_loss = 0.12634, train_acc = 0.95833, val_loss = 0.19779, val_acc = 0.92708\n",
      "[ 82/100 ] | train_loss = 0.13159, train_acc = 0.94531, val_loss = 0.11913, val_acc = 0.93750\n",
      "[ 83/100 ] | train_loss = 0.14378, train_acc = 0.94792, val_loss = 0.12586, val_acc = 0.91667\n",
      "[ 84/100 ] | train_loss = 0.14058, train_acc = 0.94531, val_loss = 0.05319, val_acc = 0.92708\n",
      "[ 85/100 ] | train_loss = 0.14506, train_acc = 0.94792, val_loss = 0.06471, val_acc = 0.93750\n",
      "[ 86/100 ] | train_loss = 0.13104, train_acc = 0.95052, val_loss = 0.17765, val_acc = 0.90625\n",
      "[ 87/100 ] | train_loss = 0.13524, train_acc = 0.94271, val_loss = 0.26989, val_acc = 0.92708\n",
      "[ 88/100 ] | train_loss = 0.12562, train_acc = 0.95833, val_loss = 0.05245, val_acc = 0.92708\n",
      "[ 89/100 ] | train_loss = 0.14015, train_acc = 0.93750, val_loss = 0.12327, val_acc = 0.91667\n",
      "[ 90/100 ] | train_loss = 0.12406, train_acc = 0.94531, val_loss = 0.15295, val_acc = 0.92708\n",
      "[ 91/100 ] | train_loss = 0.14117, train_acc = 0.94531, val_loss = 0.19349, val_acc = 0.92708\n",
      "[ 92/100 ] | train_loss = 0.12154, train_acc = 0.94792, val_loss = 0.25560, val_acc = 0.92708\n",
      "[ 93/100 ] | train_loss = 0.11719, train_acc = 0.96094, val_loss = 0.08179, val_acc = 0.93750\n",
      "[ 94/100 ] | train_loss = 0.12680, train_acc = 0.95312, val_loss = 0.09280, val_acc = 0.91667\n",
      "[ 95/100 ] | train_loss = 0.13040, train_acc = 0.94792, val_loss = 0.15855, val_acc = 0.93750\n",
      "[ 96/100 ] | train_loss = 0.13003, train_acc = 0.95312, val_loss = 0.08819, val_acc = 0.92708\n",
      "[ 97/100 ] | train_loss = 0.12099, train_acc = 0.94792, val_loss = 0.12026, val_acc = 0.92708\n",
      "[ 98/100 ] | train_loss = 0.12211, train_acc = 0.94531, val_loss = 0.04086, val_acc = 0.92708\n",
      "[ 99/100 ] | train_loss = 0.12519, train_acc = 0.95573, val_loss = 0.04273, val_acc = 0.92708\n",
      "[ 100/100 ] | train_loss = 0.10575, train_acc = 0.96615, val_loss = 0.03356, val_acc = 0.93750\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BinaryClassification(dropout=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "regularization(train_loader, val_loader, model, epochs=100, criterion=criterion, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100 ] | train_loss = 0.66753, train_acc = 0.58333, val_loss = 0.59166, val_acc = 0.76042\n",
      "[ 2/100 ] | train_loss = 0.47878, train_acc = 0.82292, val_loss = 0.24175, val_acc = 0.79167\n",
      "[ 3/100 ] | train_loss = 0.41122, train_acc = 0.85677, val_loss = 0.47515, val_acc = 0.80208\n",
      "[ 4/100 ] | train_loss = 0.37385, train_acc = 0.86458, val_loss = 0.61439, val_acc = 0.81250\n",
      "[ 5/100 ] | train_loss = 0.34675, train_acc = 0.86719, val_loss = 0.41991, val_acc = 0.82292\n",
      "[ 6/100 ] | train_loss = 0.32787, train_acc = 0.86458, val_loss = 0.25247, val_acc = 0.82292\n",
      "[ 7/100 ] | train_loss = 0.31786, train_acc = 0.86719, val_loss = 0.19771, val_acc = 0.83333\n",
      "[ 8/100 ] | train_loss = 0.30486, train_acc = 0.88021, val_loss = 0.12140, val_acc = 0.84375\n",
      "[ 9/100 ] | train_loss = 0.29751, train_acc = 0.88802, val_loss = 0.17287, val_acc = 0.85417\n",
      "[ 10/100 ] | train_loss = 0.29003, train_acc = 0.88802, val_loss = 0.27099, val_acc = 0.84375\n",
      "[ 11/100 ] | train_loss = 0.28256, train_acc = 0.89062, val_loss = 0.53172, val_acc = 0.84375\n",
      "[ 12/100 ] | train_loss = 0.27686, train_acc = 0.90625, val_loss = 0.28955, val_acc = 0.86458\n",
      "[ 13/100 ] | train_loss = 0.27269, train_acc = 0.90104, val_loss = 0.15800, val_acc = 0.87500\n",
      "[ 14/100 ] | train_loss = 0.26653, train_acc = 0.90104, val_loss = 0.21017, val_acc = 0.87500\n",
      "[ 15/100 ] | train_loss = 0.26144, train_acc = 0.90104, val_loss = 0.18372, val_acc = 0.88542\n",
      "[ 16/100 ] | train_loss = 0.25472, train_acc = 0.90365, val_loss = 0.24442, val_acc = 0.88542\n",
      "[ 17/100 ] | train_loss = 0.25237, train_acc = 0.89844, val_loss = 0.18473, val_acc = 0.88542\n",
      "[ 18/100 ] | train_loss = 0.25007, train_acc = 0.90625, val_loss = 0.44437, val_acc = 0.87500\n",
      "[ 19/100 ] | train_loss = 0.24004, train_acc = 0.90365, val_loss = 0.38142, val_acc = 0.88542\n",
      "[ 20/100 ] | train_loss = 0.23695, train_acc = 0.91146, val_loss = 0.08311, val_acc = 0.88542\n",
      "[ 21/100 ] | train_loss = 0.23267, train_acc = 0.91667, val_loss = 0.29740, val_acc = 0.88542\n",
      "[ 22/100 ] | train_loss = 0.22904, train_acc = 0.91406, val_loss = 0.08492, val_acc = 0.88542\n",
      "[ 23/100 ] | train_loss = 0.22247, train_acc = 0.91406, val_loss = 0.15180, val_acc = 0.88542\n",
      "[ 24/100 ] | train_loss = 0.21894, train_acc = 0.91667, val_loss = 0.29695, val_acc = 0.89583\n",
      "[ 25/100 ] | train_loss = 0.21640, train_acc = 0.91667, val_loss = 0.31108, val_acc = 0.89583\n",
      "[ 26/100 ] | train_loss = 0.21036, train_acc = 0.91927, val_loss = 0.28597, val_acc = 0.88542\n",
      "[ 27/100 ] | train_loss = 0.20838, train_acc = 0.91667, val_loss = 0.16288, val_acc = 0.89583\n",
      "[ 28/100 ] | train_loss = 0.20239, train_acc = 0.92448, val_loss = 0.21564, val_acc = 0.89583\n",
      "[ 29/100 ] | train_loss = 0.20065, train_acc = 0.92448, val_loss = 0.29837, val_acc = 0.89583\n",
      "[ 30/100 ] | train_loss = 0.19773, train_acc = 0.92448, val_loss = 0.17059, val_acc = 0.89583\n",
      "[ 31/100 ] | train_loss = 0.19417, train_acc = 0.92448, val_loss = 0.12546, val_acc = 0.89583\n",
      "[ 32/100 ] | train_loss = 0.19160, train_acc = 0.92969, val_loss = 0.27307, val_acc = 0.89583\n",
      "[ 33/100 ] | train_loss = 0.19016, train_acc = 0.92969, val_loss = 0.19933, val_acc = 0.90625\n",
      "[ 34/100 ] | train_loss = 0.18547, train_acc = 0.92969, val_loss = 0.13537, val_acc = 0.90625\n",
      "[ 35/100 ] | train_loss = 0.18260, train_acc = 0.92969, val_loss = 0.18494, val_acc = 0.91667\n",
      "[ 36/100 ] | train_loss = 0.17937, train_acc = 0.93229, val_loss = 0.09020, val_acc = 0.91667\n",
      "[ 37/100 ] | train_loss = 0.17672, train_acc = 0.93750, val_loss = 0.15164, val_acc = 0.92708\n",
      "[ 38/100 ] | train_loss = 0.17596, train_acc = 0.93229, val_loss = 0.31716, val_acc = 0.91667\n",
      "[ 39/100 ] | train_loss = 0.17090, train_acc = 0.93750, val_loss = 0.03387, val_acc = 0.92708\n",
      "[ 40/100 ] | train_loss = 0.16690, train_acc = 0.93229, val_loss = 0.36672, val_acc = 0.92708\n",
      "[ 41/100 ] | train_loss = 0.16333, train_acc = 0.93750, val_loss = 0.03747, val_acc = 0.92708\n",
      "[ 42/100 ] | train_loss = 0.16126, train_acc = 0.93750, val_loss = 0.07636, val_acc = 0.92708\n",
      "[ 43/100 ] | train_loss = 0.15982, train_acc = 0.93750, val_loss = 0.27101, val_acc = 0.92708\n",
      "[ 44/100 ] | train_loss = 0.15633, train_acc = 0.94271, val_loss = 0.22337, val_acc = 0.93750\n",
      "[ 45/100 ] | train_loss = 0.15434, train_acc = 0.94010, val_loss = 0.42805, val_acc = 0.92708\n",
      "[ 46/100 ] | train_loss = 0.15305, train_acc = 0.94531, val_loss = 0.25106, val_acc = 0.92708\n",
      "[ 47/100 ] | train_loss = 0.15194, train_acc = 0.93750, val_loss = 0.20232, val_acc = 0.92708\n",
      "[ 48/100 ] | train_loss = 0.14776, train_acc = 0.94531, val_loss = 0.19606, val_acc = 0.92708\n",
      "[ 49/100 ] | train_loss = 0.14376, train_acc = 0.94792, val_loss = 0.18236, val_acc = 0.93750\n",
      "[ 50/100 ] | train_loss = 0.14183, train_acc = 0.94531, val_loss = 0.08086, val_acc = 0.93750\n",
      "[ 51/100 ] | train_loss = 0.14013, train_acc = 0.94792, val_loss = 0.09906, val_acc = 0.93750\n",
      "[ 52/100 ] | train_loss = 0.13949, train_acc = 0.94010, val_loss = 0.04453, val_acc = 0.93750\n",
      "[ 53/100 ] | train_loss = 0.13502, train_acc = 0.94792, val_loss = 0.06435, val_acc = 0.93750\n",
      "[ 54/100 ] | train_loss = 0.13310, train_acc = 0.95052, val_loss = 0.11828, val_acc = 0.93750\n",
      "[ 55/100 ] | train_loss = 0.13237, train_acc = 0.95312, val_loss = 0.06316, val_acc = 0.93750\n",
      "[ 56/100 ] | train_loss = 0.12895, train_acc = 0.94792, val_loss = 0.10719, val_acc = 0.93750\n",
      "[ 57/100 ] | train_loss = 0.12937, train_acc = 0.95312, val_loss = 0.17800, val_acc = 0.93750\n",
      "[ 58/100 ] | train_loss = 0.12652, train_acc = 0.95312, val_loss = 0.08567, val_acc = 0.94792\n",
      "[ 59/100 ] | train_loss = 0.12348, train_acc = 0.95833, val_loss = 0.12119, val_acc = 0.93750\n",
      "[ 60/100 ] | train_loss = 0.12089, train_acc = 0.96354, val_loss = 0.16928, val_acc = 0.94792\n",
      "[ 61/100 ] | train_loss = 0.12128, train_acc = 0.95312, val_loss = 0.07766, val_acc = 0.94792\n",
      "[ 62/100 ] | train_loss = 0.11843, train_acc = 0.95312, val_loss = 0.11984, val_acc = 0.93750\n",
      "[ 63/100 ] | train_loss = 0.11908, train_acc = 0.95573, val_loss = 0.17019, val_acc = 0.93750\n",
      "[ 64/100 ] | train_loss = 0.11749, train_acc = 0.96094, val_loss = 0.07183, val_acc = 0.94792\n",
      "[ 65/100 ] | train_loss = 0.11518, train_acc = 0.96354, val_loss = 0.01741, val_acc = 0.93750\n",
      "[ 66/100 ] | train_loss = 0.11146, train_acc = 0.96094, val_loss = 0.13361, val_acc = 0.93750\n",
      "[ 67/100 ] | train_loss = 0.11126, train_acc = 0.95833, val_loss = 0.08645, val_acc = 0.93750\n",
      "[ 68/100 ] | train_loss = 0.10956, train_acc = 0.95833, val_loss = 0.21176, val_acc = 0.94792\n",
      "[ 69/100 ] | train_loss = 0.10879, train_acc = 0.95573, val_loss = 0.09544, val_acc = 0.94792\n",
      "[ 70/100 ] | train_loss = 0.10500, train_acc = 0.95833, val_loss = 0.09494, val_acc = 0.94792\n",
      "[ 71/100 ] | train_loss = 0.10772, train_acc = 0.96875, val_loss = 0.15299, val_acc = 0.94792\n",
      "[ 72/100 ] | train_loss = 0.10490, train_acc = 0.96094, val_loss = 0.01728, val_acc = 0.93750\n",
      "[ 73/100 ] | train_loss = 0.10276, train_acc = 0.95573, val_loss = 0.14847, val_acc = 0.94792\n",
      "[ 74/100 ] | train_loss = 0.09982, train_acc = 0.96354, val_loss = 0.09070, val_acc = 0.94792\n",
      "[ 75/100 ] | train_loss = 0.09659, train_acc = 0.96615, val_loss = 0.04186, val_acc = 0.94792\n",
      "[ 76/100 ] | train_loss = 0.09780, train_acc = 0.96094, val_loss = 0.10915, val_acc = 0.94792\n",
      "[ 77/100 ] | train_loss = 0.09437, train_acc = 0.96615, val_loss = 0.11262, val_acc = 0.94792\n",
      "[ 78/100 ] | train_loss = 0.09456, train_acc = 0.96615, val_loss = 0.13800, val_acc = 0.94792\n",
      "[ 79/100 ] | train_loss = 0.09657, train_acc = 0.96354, val_loss = 0.11461, val_acc = 0.94792\n",
      "[ 80/100 ] | train_loss = 0.09527, train_acc = 0.96094, val_loss = 0.01184, val_acc = 0.94792\n",
      "[ 81/100 ] | train_loss = 0.09231, train_acc = 0.97396, val_loss = 0.15919, val_acc = 0.94792\n",
      "[ 82/100 ] | train_loss = 0.09078, train_acc = 0.96354, val_loss = 0.06943, val_acc = 0.94792\n",
      "[ 83/100 ] | train_loss = 0.08938, train_acc = 0.96354, val_loss = 0.06820, val_acc = 0.94792\n",
      "[ 84/100 ] | train_loss = 0.08827, train_acc = 0.96354, val_loss = 0.13379, val_acc = 0.94792\n",
      "[ 85/100 ] | train_loss = 0.08487, train_acc = 0.97135, val_loss = 0.02367, val_acc = 0.94792\n",
      "[ 86/100 ] | train_loss = 0.08527, train_acc = 0.97656, val_loss = 0.04776, val_acc = 0.94792\n",
      "[ 87/100 ] | train_loss = 0.08542, train_acc = 0.97656, val_loss = 0.01501, val_acc = 0.94792\n",
      "[ 88/100 ] | train_loss = 0.08508, train_acc = 0.97135, val_loss = 0.15164, val_acc = 0.94792\n",
      "[ 89/100 ] | train_loss = 0.08673, train_acc = 0.97656, val_loss = 0.11387, val_acc = 0.94792\n",
      "[ 90/100 ] | train_loss = 0.08334, train_acc = 0.97396, val_loss = 0.09561, val_acc = 0.94792\n",
      "[ 91/100 ] | train_loss = 0.07948, train_acc = 0.97396, val_loss = 0.04656, val_acc = 0.94792\n",
      "[ 92/100 ] | train_loss = 0.07928, train_acc = 0.97656, val_loss = 0.11440, val_acc = 0.94792\n",
      "[ 93/100 ] | train_loss = 0.07850, train_acc = 0.97656, val_loss = 0.08665, val_acc = 0.94792\n",
      "[ 94/100 ] | train_loss = 0.07700, train_acc = 0.97917, val_loss = 0.08377, val_acc = 0.94792\n",
      "[ 95/100 ] | train_loss = 0.07685, train_acc = 0.98177, val_loss = 0.04710, val_acc = 0.94792\n",
      "[ 96/100 ] | train_loss = 0.07710, train_acc = 0.98177, val_loss = 0.07110, val_acc = 0.94792\n",
      "[ 97/100 ] | train_loss = 0.07526, train_acc = 0.97917, val_loss = 0.02475, val_acc = 0.94792\n",
      "[ 98/100 ] | train_loss = 0.07415, train_acc = 0.98438, val_loss = 0.05422, val_acc = 0.94792\n",
      "[ 99/100 ] | train_loss = 0.07177, train_acc = 0.98177, val_loss = 0.02207, val_acc = 0.94792\n",
      "[ 100/100 ] | train_loss = 0.07121, train_acc = 0.98177, val_loss = 0.05195, val_acc = 0.94792\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BinaryClassification(batch_norm=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "regularization(train_loader, val_loader, model, epochs=100, criterion=criterion, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac3da779536756720bc930bbdcbe3b303a716c4190960bb8b007750e7b6b7c5d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
