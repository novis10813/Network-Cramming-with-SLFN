{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath) -> None:\n",
    "        \n",
    "        # load csv data\n",
    "        data = pd.read_csv(filepath, header=None)\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "        \n",
    "        # feature scaling\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "        # convert to tensors\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('train_all_0.csv')\n",
    "\n",
    "# create data indices for train val split\n",
    "data_size = len(dataset)\n",
    "indices = list(range(data_size))\n",
    "split = int(np.floor(0.2 * data_size))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# create data loader\n",
    "train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=16, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(12, 128) \n",
    "        self.layer_out = nn.Linear(128, 2) \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline based on page 49\n",
    "def P49_train(train_loader=train_loader, val_loader=val_loader, model=None, epochs=None, criterion=None, optimizer=None, loss_threshold=0.5):\n",
    "    '''When iterating over 50 or validation loss smaller than specific number, the function will stop'''\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            \n",
    "            x, y = batch\n",
    "            \n",
    "            logits = model(x.to(device))\n",
    "            loss = criterion(logits, y.to(device))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()\n",
    "            train_loss.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "        \n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        train_acc = sum(train_accs) / len(train_accs)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        valid_loss = []\n",
    "        valid_accs = []\n",
    "        \n",
    "        for batch in val_loader:\n",
    "            imgs, labels = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model(imgs.to(device))\n",
    "                \n",
    "                acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "                \n",
    "                valid_loss.append(loss.item())\n",
    "                valid_accs.append(acc)\n",
    "        \n",
    "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "        \n",
    "        print(f'[ {epoch+1}/{epochs} ] | train_loss = {train_loss:.5f}, train_acc = {train_acc:.5f}, val_loss = {valid_loss:.5f}, val_acc = {valid_acc:.5f}')\n",
    "        \n",
    "        if epoch+1 >= 50:\n",
    "            print(\"It's over 50 epochs, stop training\")    \n",
    "            break\n",
    "        \n",
    "        if train_loss < loss_threshold:\n",
    "            print('The training loss is smaller than what you want, stop training')\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline based on page 50\n",
    "def P50_train(train_loader=train_loader, val_loader=val_loader, model=None, epochs=None, criterion=None, optimizer=None, loss_threshold=0.5, eta_threshold=0.008):\n",
    "    '''\n",
    "    When iterating over 50 or validation loss smaller than specific number, the function will stop.\n",
    "    Also, if the new loss is larger than the previous one, it will compare the learning rate and the threshold.\n",
    "    if the learning rate is larger than the threshold, use the old weight and the learning rate will multiply 0.7 and do bp one more time.\n",
    "    if not, then stop training.\n",
    "    However if the new loss is smaller than the previous one, update w to new weight and multiply learning rate by 1.2, and going through new loop\n",
    "    '''\n",
    "    \n",
    "    previous_train_loss = 10000    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        previous_model_params = model.state_dict()\n",
    "        stop_training = False\n",
    "        \n",
    "        while optimizer.param_groups[0]['lr'] > eta_threshold:\n",
    "            \n",
    "            train_loss = []\n",
    "            train_accs = []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                \n",
    "                x, y = batch\n",
    "                \n",
    "                logits = model(x.to(device))\n",
    "                loss = criterion(logits, y.to(device))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()\n",
    "                train_loss.append(loss.item())\n",
    "                train_accs.append(acc)\n",
    "            \n",
    "            train_loss = sum(train_loss) / len(train_loss)\n",
    "            train_acc = sum(train_accs) / len(train_accs)\n",
    "            \n",
    "            if train_loss < previous_train_loss:\n",
    "                optimizer.param_groups[0]['lr'] *= 1.2\n",
    "                previous_train_loss = train_loss\n",
    "                print(f'The previous training loss is: {previous_train_loss}')\n",
    "                break\n",
    "            \n",
    "            optimizer.param_groups[0]['lr'] *= 0.7\n",
    "            model.load_state_dict(previous_model_params)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'lr shrinking!, now the lr is: {current_lr}')\n",
    "            \n",
    "        else:\n",
    "            stop_training = True\n",
    "        \n",
    "        # Use try and except to detect whether the eta_threshold is set too high initially\n",
    "        try:        \n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_accs = []\n",
    "            \n",
    "            for batch in val_loader:\n",
    "                imgs, labels = batch\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = model(imgs.to(device))\n",
    "                    \n",
    "                    acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "                    valid_loss.append(loss.item())\n",
    "                    valid_accs.append(acc)\n",
    "            \n",
    "            valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "            valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "        \n",
    "            print(f'[ {epoch+1}/{epochs} ] | train_loss = {train_loss:.5f}, train_acc = {train_acc:.5f}, val_loss = {valid_loss:.5f}, val_acc = {valid_acc:.5f}')\n",
    "            \n",
    "        except UnboundLocalError:\n",
    "            print('Your eta_threshold is setting higher than your learning rate. Reset it with lower one!')\n",
    "        \n",
    "        # stopping criterion\n",
    "        if stop_training:\n",
    "            print('Learning rate is smaller than the threshold, stop training.')\n",
    "            break\n",
    "        \n",
    "        if epoch+1 >= 50:\n",
    "            print('It over 50 epochs, stop training.')\n",
    "            break\n",
    "        \n",
    "        if train_loss < loss_threshold:\n",
    "            print('The training loss is smaller than what you want, stop training.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline based on page 58\n",
    "def P58_train(train_loader=train_loader, val_loader=val_loader, model=None, criterion=None, optimizer=None, loss_threshold=0.5, eta_threshold=0.008):\n",
    "    '''\n",
    "    Almost the same as P50, but without training epochs.\n",
    "    '''\n",
    "    \n",
    "    previous_train_loss = 10000    \n",
    "\n",
    "    for epoch in itertools.count():\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        previous_model_params = model.state_dict()\n",
    "        stop_training = False\n",
    "        \n",
    "        while optimizer.param_groups[0]['lr'] > eta_threshold:\n",
    "            \n",
    "            train_loss = []\n",
    "            train_accs = []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                \n",
    "                x, y = batch\n",
    "                \n",
    "                logits = model(x.to(device))\n",
    "                loss = criterion(logits, y.to(device))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                acc = (logits.argmax(dim=-1) == y.to(device)).float().mean()\n",
    "                train_loss.append(loss.item())\n",
    "                train_accs.append(acc)\n",
    "            \n",
    "            train_loss = sum(train_loss) / len(train_loss)\n",
    "            train_acc = sum(train_accs) / len(train_accs)\n",
    "            \n",
    "            if train_loss < previous_train_loss:\n",
    "                optimizer.param_groups[0]['lr'] *= 1.2\n",
    "                previous_train_loss = train_loss\n",
    "                print(f'The previous training loss is: {previous_train_loss}')\n",
    "                break\n",
    "            \n",
    "            optimizer.param_groups[0]['lr'] *= 0.7\n",
    "            model.load_state_dict(previous_model_params)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'lr shrinking!, now the lr is: {current_lr}')\n",
    "            \n",
    "        else:\n",
    "            stop_training = True\n",
    "        \n",
    "        # Use try and except to detect whether the eta_threshold is set too high initially\n",
    "        try:        \n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_accs = []\n",
    "            \n",
    "            for batch in val_loader:\n",
    "                imgs, labels = batch\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = model(imgs.to(device))\n",
    "                    \n",
    "                    acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "                    valid_loss.append(loss.item())\n",
    "                    valid_accs.append(acc)\n",
    "            \n",
    "            valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "            valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "        \n",
    "            print(f'[ {epoch+1}/{epochs} ] | train_loss = {train_loss:.5f}, train_acc = {train_acc:.5f}, val_loss = {valid_loss:.5f}, val_acc = {valid_acc:.5f}')\n",
    "            \n",
    "        except UnboundLocalError:\n",
    "            print('Your eta_threshold is setting higher than your learning rate. Reset it with lower one!')\n",
    "        \n",
    "        # stopping criterion\n",
    "        if stop_training:\n",
    "            print('Learning rate is smaller than the threshold, stop training.')\n",
    "            break\n",
    "        \n",
    "        if train_loss < loss_threshold:\n",
    "            print('The training loss is smaller than what you want, stop training.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/100 ] | train_loss = 0.54745, train_acc = 0.79427, val_loss = 0.34997, val_acc = 0.79167\n",
      "[ 2/100 ] | train_loss = 0.42227, train_acc = 0.83073, val_loss = 0.42259, val_acc = 0.82292\n",
      "[ 3/100 ] | train_loss = 0.36963, train_acc = 0.85677, val_loss = 0.32141, val_acc = 0.82292\n",
      "[ 4/100 ] | train_loss = 0.34281, train_acc = 0.85677, val_loss = 0.29560, val_acc = 0.84375\n",
      "[ 5/100 ] | train_loss = 0.32344, train_acc = 0.86719, val_loss = 0.22710, val_acc = 0.84375\n",
      "[ 6/100 ] | train_loss = 0.30918, train_acc = 0.87500, val_loss = 0.41587, val_acc = 0.84375\n",
      "[ 7/100 ] | train_loss = 0.29792, train_acc = 0.86979, val_loss = 0.13412, val_acc = 0.86458\n",
      "[ 8/100 ] | train_loss = 0.29051, train_acc = 0.88281, val_loss = 0.23659, val_acc = 0.87500\n",
      "[ 9/100 ] | train_loss = 0.28087, train_acc = 0.88281, val_loss = 0.38199, val_acc = 0.86458\n",
      "[ 10/100 ] | train_loss = 0.27050, train_acc = 0.89323, val_loss = 0.24689, val_acc = 0.87500\n",
      "[ 11/100 ] | train_loss = 0.26381, train_acc = 0.89844, val_loss = 0.15098, val_acc = 0.86458\n",
      "[ 12/100 ] | train_loss = 0.25596, train_acc = 0.90365, val_loss = 0.31771, val_acc = 0.87500\n",
      "[ 13/100 ] | train_loss = 0.25036, train_acc = 0.90365, val_loss = 0.14416, val_acc = 0.87500\n",
      "[ 14/100 ] | train_loss = 0.24569, train_acc = 0.90104, val_loss = 0.59910, val_acc = 0.86458\n",
      "[ 15/100 ] | train_loss = 0.23687, train_acc = 0.90885, val_loss = 0.22858, val_acc = 0.88542\n",
      "[ 16/100 ] | train_loss = 0.23450, train_acc = 0.91667, val_loss = 0.29798, val_acc = 0.89583\n",
      "[ 17/100 ] | train_loss = 0.22485, train_acc = 0.91406, val_loss = 0.15618, val_acc = 0.87500\n",
      "[ 18/100 ] | train_loss = 0.21965, train_acc = 0.91406, val_loss = 0.14739, val_acc = 0.88542\n",
      "[ 19/100 ] | train_loss = 0.21473, train_acc = 0.91667, val_loss = 0.37024, val_acc = 0.88542\n",
      "[ 20/100 ] | train_loss = 0.21154, train_acc = 0.91927, val_loss = 0.13335, val_acc = 0.89583\n",
      "[ 21/100 ] | train_loss = 0.20671, train_acc = 0.92448, val_loss = 0.14500, val_acc = 0.88542\n",
      "[ 22/100 ] | train_loss = 0.20075, train_acc = 0.92708, val_loss = 0.15546, val_acc = 0.89583\n",
      "[ 23/100 ] | train_loss = 0.19763, train_acc = 0.92969, val_loss = 0.10245, val_acc = 0.89583\n",
      "The training loss is smaller than what you want, stop training\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BinaryClassification().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "P49_train(train_loader, val_loader, model, epochs, criterion, optimizer, loss_threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous training loss is: 0.508815407132109\n",
      "[ 1/100 ] | train_loss = 0.50882, train_acc = 0.80729, val_loss = 0.24727, val_acc = 0.79167\n",
      "The previous training loss is: 0.4118443491558234\n",
      "[ 2/100 ] | train_loss = 0.41184, train_acc = 0.82552, val_loss = 0.26101, val_acc = 0.82292\n",
      "The previous training loss is: 0.35450122070809204\n",
      "[ 3/100 ] | train_loss = 0.35450, train_acc = 0.85677, val_loss = 0.30807, val_acc = 0.85417\n",
      "The previous training loss is: 0.32634108513593674\n",
      "[ 4/100 ] | train_loss = 0.32634, train_acc = 0.86719, val_loss = 0.22383, val_acc = 0.88542\n",
      "The previous training loss is: 0.3105933458233873\n",
      "[ 5/100 ] | train_loss = 0.31059, train_acc = 0.88802, val_loss = 0.32867, val_acc = 0.89583\n",
      "The previous training loss is: 0.2868718837077419\n",
      "[ 6/100 ] | train_loss = 0.28687, train_acc = 0.90104, val_loss = 0.09822, val_acc = 0.87500\n",
      "The previous training loss is: 0.27396557324876386\n",
      "[ 7/100 ] | train_loss = 0.27397, train_acc = 0.90104, val_loss = 0.29306, val_acc = 0.89583\n",
      "The previous training loss is: 0.2567768068984151\n",
      "[ 8/100 ] | train_loss = 0.25678, train_acc = 0.90104, val_loss = 0.35329, val_acc = 0.89583\n",
      "The previous training loss is: 0.23834709885219732\n",
      "[ 9/100 ] | train_loss = 0.23835, train_acc = 0.91406, val_loss = 0.29764, val_acc = 0.90625\n",
      "The previous training loss is: 0.22329298251618943\n",
      "[ 10/100 ] | train_loss = 0.22329, train_acc = 0.91927, val_loss = 0.15657, val_acc = 0.87500\n",
      "The previous training loss is: 0.22065145584444204\n",
      "[ 11/100 ] | train_loss = 0.22065, train_acc = 0.90365, val_loss = 0.15526, val_acc = 0.88542\n",
      "The previous training loss is: 0.19737442520757517\n",
      "[ 12/100 ] | train_loss = 0.19737, train_acc = 0.92448, val_loss = 0.16199, val_acc = 0.88542\n",
      "The previous training loss is: 0.1796218555731078\n",
      "[ 13/100 ] | train_loss = 0.17962, train_acc = 0.92708, val_loss = 0.12011, val_acc = 0.90625\n",
      "lr shrinking!, now the lr is 0.007489524376535036\n",
      "lr shrinking!, now the lr is 0.0052426670635745245\n",
      "The previous training loss is: 0.16340440092608333\n",
      "[ 14/100 ] | train_loss = 0.16340, train_acc = 0.94531, val_loss = 0.15545, val_acc = 0.90625\n",
      "The previous training loss is: 0.13305759460975727\n",
      "[ 15/100 ] | train_loss = 0.13306, train_acc = 0.94271, val_loss = 0.05918, val_acc = 0.90625\n",
      "lr shrinking!, now the lr is 0.00528460840008312\n",
      "The previous training loss is: 0.12494320612555991\n",
      "[ 16/100 ] | train_loss = 0.12494, train_acc = 0.95052, val_loss = 0.10232, val_acc = 0.89583\n",
      "lr shrinking!, now the lr is 0.00443907105606982\n",
      "The previous training loss is: 0.11838068219367415\n",
      "[ 17/100 ] | train_loss = 0.11838, train_acc = 0.94531, val_loss = 0.01284, val_acc = 0.90625\n",
      "The previous training loss is: 0.1161433388479054\n",
      "[ 18/100 ] | train_loss = 0.11614, train_acc = 0.95833, val_loss = 0.09170, val_acc = 0.91667\n",
      "The previous training loss is: 0.09358448876688878\n",
      "[ 19/100 ] | train_loss = 0.09358, train_acc = 0.96875, val_loss = 0.09188, val_acc = 0.92708\n",
      "lr shrinking!, now the lr is 0.005369500349422053\n",
      "The previous training loss is: 0.08631341244714956\n",
      "[ 20/100 ] | train_loss = 0.08631, train_acc = 0.97396, val_loss = 0.12617, val_acc = 0.91667\n",
      "lr shrinking!, now the lr is 0.004510380293514525\n",
      "lr shrinking!, now the lr is 0.0031572662054601673\n",
      "The previous training loss is: 0.06850515973443787\n",
      "[ 21/100 ] | train_loss = 0.06851, train_acc = 0.97656, val_loss = 0.04578, val_acc = 0.92708\n",
      "The previous training loss is: 0.06224040923795352\n",
      "[ 22/100 ] | train_loss = 0.06224, train_acc = 0.98698, val_loss = 0.05364, val_acc = 0.92708\n",
      "lr shrinking!, now the lr is 0.0031825243351038483\n",
      "The previous training loss is: 0.06081160765218859\n",
      "[ 23/100 ] | train_loss = 0.06081, train_acc = 0.98698, val_loss = 0.09661, val_acc = 0.91667\n",
      "The previous training loss is: 0.05810129811288789\n",
      "[ 24/100 ] | train_loss = 0.05810, train_acc = 0.98958, val_loss = 0.00577, val_acc = 0.93750\n",
      "lr shrinking!, now the lr is 0.0032079845297846785\n",
      "lr shrinking!, now the lr is 0.002245589170849275\n",
      "The previous training loss is: 0.04875329282367602\n",
      "[ 25/100 ] | train_loss = 0.04875, train_acc = 0.98958, val_loss = 0.01024, val_acc = 0.93750\n",
      "The training loss is smaller than what you want, stop training.\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BinaryClassification().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "P50_train(train_loader, val_loader, model, epochs, criterion, optimizer, loss_threshold=0.05, eta_threshold=9e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous training loss is: 0.6209744065999985\n",
      "[ 1/100 ] | train_loss = 0.62097, train_acc = 0.66667, val_loss = 0.43850, val_acc = 0.79167\n",
      "The previous training loss is: 0.44926222041249275\n",
      "[ 2/100 ] | train_loss = 0.44926, train_acc = 0.81250, val_loss = 0.48079, val_acc = 0.81250\n",
      "The previous training loss is: 0.37369591121872264\n",
      "[ 3/100 ] | train_loss = 0.37370, train_acc = 0.85677, val_loss = 0.38961, val_acc = 0.84375\n",
      "The previous training loss is: 0.3326238188892603\n",
      "[ 4/100 ] | train_loss = 0.33262, train_acc = 0.87500, val_loss = 0.27778, val_acc = 0.86458\n",
      "The previous training loss is: 0.30780113612612087\n",
      "[ 5/100 ] | train_loss = 0.30780, train_acc = 0.86979, val_loss = 0.28548, val_acc = 0.87500\n",
      "The previous training loss is: 0.2896661659081777\n",
      "[ 6/100 ] | train_loss = 0.28967, train_acc = 0.87760, val_loss = 0.28282, val_acc = 0.87500\n",
      "The previous training loss is: 0.27271145830551785\n",
      "[ 7/100 ] | train_loss = 0.27271, train_acc = 0.89323, val_loss = 0.35666, val_acc = 0.87500\n",
      "The previous training loss is: 0.25803521554917097\n",
      "[ 8/100 ] | train_loss = 0.25804, train_acc = 0.89844, val_loss = 0.16415, val_acc = 0.86458\n",
      "The previous training loss is: 0.2467426989848415\n",
      "[ 9/100 ] | train_loss = 0.24674, train_acc = 0.90104, val_loss = 0.35693, val_acc = 0.88542\n",
      "The previous training loss is: 0.22870593052357435\n",
      "[ 10/100 ] | train_loss = 0.22871, train_acc = 0.91927, val_loss = 0.21103, val_acc = 0.87500\n",
      "The previous training loss is: 0.21345739687482515\n",
      "[ 11/100 ] | train_loss = 0.21346, train_acc = 0.92188, val_loss = 0.64664, val_acc = 0.85417\n",
      "The previous training loss is: 0.21294144509981075\n",
      "[ 12/100 ] | train_loss = 0.21294, train_acc = 0.92708, val_loss = 0.33941, val_acc = 0.88542\n",
      "The previous training loss is: 0.2007243918875853\n",
      "[ 13/100 ] | train_loss = 0.20072, train_acc = 0.92188, val_loss = 0.39447, val_acc = 0.90625\n",
      "lr shrinking!, now the lr is: 0.007489524376535036\n",
      "lr shrinking!, now the lr is: 0.0052426670635745245\n",
      "The previous training loss is: 0.16332532853508988\n",
      "[ 14/100 ] | train_loss = 0.16333, train_acc = 0.94010, val_loss = 0.25930, val_acc = 0.90625\n",
      "The previous training loss is: 0.13097705009082952\n",
      "[ 15/100 ] | train_loss = 0.13098, train_acc = 0.94531, val_loss = 0.12936, val_acc = 0.92708\n",
      "lr shrinking!, now the lr is: 0.00528460840008312\n",
      "The previous training loss is: 0.12503934387738505\n",
      "[ 16/100 ] | train_loss = 0.12504, train_acc = 0.94792, val_loss = 0.24911, val_acc = 0.90625\n",
      "The previous training loss is: 0.11753982910886407\n",
      "[ 17/100 ] | train_loss = 0.11754, train_acc = 0.94531, val_loss = 0.11394, val_acc = 0.92708\n",
      "The previous training loss is: 0.10594059185435374\n",
      "[ 18/100 ] | train_loss = 0.10594, train_acc = 0.96615, val_loss = 0.29393, val_acc = 0.88542\n",
      "lr shrinking!, now the lr is: 0.006392262320740541\n",
      "lr shrinking!, now the lr is: 0.004474583624518379\n",
      "The previous training loss is: 0.08357263651366036\n",
      "[ 19/100 ] | train_loss = 0.08357, train_acc = 0.98177, val_loss = 0.01937, val_acc = 0.90625\n",
      "The previous training loss is: 0.08031964624145378\n",
      "[ 20/100 ] | train_loss = 0.08032, train_acc = 0.97656, val_loss = 0.28671, val_acc = 0.91667\n",
      "lr shrinking!, now the lr is: 0.004510380293514525\n",
      "lr shrinking!, now the lr is: 0.0031572662054601673\n",
      "The previous training loss is: 0.07014046163142969\n",
      "[ 21/100 ] | train_loss = 0.07014, train_acc = 0.97917, val_loss = 0.05085, val_acc = 0.90625\n",
      "The previous training loss is: 0.06583911298851793\n",
      "[ 22/100 ] | train_loss = 0.06584, train_acc = 0.98698, val_loss = 0.03252, val_acc = 0.90625\n",
      "The previous training loss is: 0.06418909376952797\n",
      "[ 23/100 ] | train_loss = 0.06419, train_acc = 0.98438, val_loss = 0.05314, val_acc = 0.90625\n",
      "The previous training loss is: 0.06341711812031765\n",
      "[ 24/100 ] | train_loss = 0.06342, train_acc = 0.98438, val_loss = 0.17085, val_acc = 0.91667\n",
      "The previous training loss is: 0.06164636082636813\n",
      "[ 25/100 ] | train_loss = 0.06165, train_acc = 0.98438, val_loss = 0.01982, val_acc = 0.90625\n",
      "lr shrinking!, now the lr is: 0.00549940205105945\n",
      "lr shrinking!, now the lr is: 0.0038495814357416146\n",
      "The previous training loss is: 0.06094131884553159\n",
      "[ 26/100 ] | train_loss = 0.06094, train_acc = 0.97917, val_loss = 0.02800, val_acc = 0.90625\n",
      "The previous training loss is: 0.05031216096055383\n",
      "[ 27/100 ] | train_loss = 0.05031, train_acc = 0.98438, val_loss = 0.04459, val_acc = 0.89583\n",
      "lr shrinking!, now the lr is: 0.003880378087227547\n",
      "The previous training loss is: 0.043596986986813135\n",
      "[ 28/100 ] | train_loss = 0.04360, train_acc = 0.99219, val_loss = 0.03433, val_acc = 0.90625\n",
      "The training loss is smaller than what you want, stop training.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BinaryClassification().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "P58_train(train_loader, val_loader, model, criterion, optimizer, loss_threshold=0.05, eta_threshold=9e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac3da779536756720bc930bbdcbe3b303a716c4190960bb8b007750e7b6b7c5d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
